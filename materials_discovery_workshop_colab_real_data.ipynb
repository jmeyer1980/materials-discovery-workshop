{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "materials-discovery-workshop"
   },
   "source": [
    "# Materials Discovery Workshop - Google Colab Edition with Materials Project Integration\n",
    "\n",
    "This interactive notebook demonstrates how machine learning can accelerate materials discovery by learning patterns from existing alloy compositions and generating new ones.\n",
    "\n",
    "**New Feature**: Integration with the Materials Project database for real materials data!\n",
    "\n",
    "**Workshop Goals:**\n",
    "- Understand how variational autoencoders (VAEs) can model materials data\n",
    "- Learn to generate new alloy compositions using ML\n",
    "- Explore materials clustering and property analysis\n",
    "- See how AI can accelerate materials R&D\n",
    "- **NEW**: Use real materials data from Materials Project\n",
    "\n",
    "**What you'll need:**\n",
    "- Basic understanding of alloys and material properties\n",
    "- Curiosity about how ML can help with materials science\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "synthesizability-predictor-code"
   },
   "outputs": [],
   "source": [
    "# Synthesizability Prediction Module for Materials Discovery Workshop\n",
    "# This code is embedded directly in the notebook for Colab compatibility\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Mock ICSD data for demonstration (in practice, this would come from actual ICSD database)\n",
    "def generate_mock_icsd_data(n_samples: int = 1000) -> pd.DataFrame:\n",
    "    \"\"\"Generate mock ICSD data representing experimentally synthesized materials.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    data = []\n",
    "    for i in range(n_samples):\n",
    "        formation_energy = np.random.normal(-2.0, 1.5)\n",
    "        formation_energy = np.clip(formation_energy, -8, 2)\n",
    "        \n",
    "        band_gap = np.random.exponential(1.0)\n",
    "        band_gap = np.clip(band_gap, 0, 8)\n",
    "        \n",
    "        energy_above_hull = np.random.exponential(0.05)\n",
    "        energy_above_hull = np.clip(energy_above_hull, 0, 0.5)\n",
    "        \n",
    "        electronegativity = np.random.normal(1.8, 0.5)\n",
    "        electronegativity = np.clip(electronegativity, 0.7, 2.5)\n",
    "        \n",
    "        atomic_radius = np.random.normal(1.4, 0.3)\n",
    "        atomic_radius = np.clip(atomic_radius, 0.8, 2.2)\n",
    "        \n",
    "        nsites = np.random.randint(1, 20)\n",
    "        density = np.random.normal(6.0, 3.0)\n",
    "        density = np.clip(density, 2, 25)\n",
    "        \n",
    "        data.append({\n",
    "            'formation_energy_per_atom': formation_energy,\n",
    "            'band_gap': band_gap,\n",
    "            'energy_above_hull': energy_above_hull,\n",
    "            'electronegativity': electronegativity,\n",
    "            'atomic_radius': atomic_radius,\n",
    "            'nsites': nsites,\n",
    "            'density': density,\n",
    "            'synthesizable': 1\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def generate_mock_mp_only_data(n_samples: int = 1000) -> pd.DataFrame:\n",
    "    \"\"\"Generate mock MP-only data representing theoretically predicted materials.\"\"\"\n",
    "    np.random.seed(123)\n",
    "    \n",
    "    data = []\n",
    "    for i in range(n_samples):\n",
    "        formation_energy = np.random.normal(-0.5, 2.0)\n",
    "        formation_energy = np.clip(formation_energy, -8, 4)\n",
    "        \n",
    "        band_gap = np.random.exponential(2.0)\n",
    "        band_gap = np.clip(band_gap, 0, 12)\n",
    "        \n",
    "        energy_above_hull = np.random.exponential(0.2)\n",
    "        energy_above_hull = np.clip(energy_above_hull, 0, 1.0)\n",
    "        \n",
    "        electronegativity = np.random.normal(1.6, 0.8)\n",
    "        electronegativity = np.clip(electronegativity, 0.5, 3.0)\n",
    "        \n",
    "        atomic_radius = np.random.normal(1.5, 0.5)\n",
    "        atomic_radius = np.clip(atomic_radius, 0.6, 2.8)\n",
    "        \n",
    "        nsites = np.random.randint(1, 50)\n",
    "        density = np.random.normal(8.0, 4.0)\n",
    "        density = np.clip(density, 1, 30)\n",
    "        \n",
    "        data.append({\n",
    "            'formation_energy_per_atom': formation_energy,\n",
    "            'band_gap': band_gap,\n",
    "            'energy_above_hull': energy_above_hull,\n",
    "            'electronegativity': electronegativity,\n",
    "            'atomic_radius': atomic_radius,\n",
    "            'nsites': nsites,\n",
    "            'density': density,\n",
    "            'synthesizable': 0\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "class SynthesizabilityClassifier:\n",
    "    \"\"\"ML-based classifier for predicting material synthesizability.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_type: str = 'random_forest'):\n",
    "        self.model_type = model_type\n",
    "        self.model = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_columns = [\n",
    "            'formation_energy_per_atom', 'band_gap', 'energy_above_hull',\n",
    "            'electronegativity', 'atomic_radius', 'nsites', 'density'\n",
    "        ]\n",
    "        self.is_trained = False\n",
    "    \n",
    "    def prepare_training_data(self) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"Prepare training data from ICSD and MP-only materials.\"\"\"\n",
    "        icsd_data = generate_mock_icsd_data(1500)\n",
    "        mp_only_data = generate_mock_mp_only_data(1500)\n",
    "        \n",
    "        training_data = pd.concat([icsd_data, mp_only_data], ignore_index=True)\n",
    "        training_data = training_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        \n",
    "        X = training_data[self.feature_columns]\n",
    "        y = training_data['synthesizable']\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def train(self, test_size: float = 0.2):\n",
    "        \"\"\"Train the synthesizability classifier.\"\"\"\n",
    "        X, y = self.prepare_training_data()\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        if self.model_type == 'random_forest':\n",
    "            self.model = RandomForestClassifier(\n",
    "                n_estimators=200, max_depth=10, min_samples_split=5,\n",
    "                min_samples_leaf=2, random_state=42, n_jobs=-1\n",
    "            )\n",
    "        elif self.model_type == 'gradient_boosting':\n",
    "            self.model = GradientBoostingClassifier(\n",
    "                n_estimators=200, max_depth=6, learning_rate=0.1, random_state=42\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model type: {self.model_type}\")\n",
    "        \n",
    "        self.model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        y_pred = self.model.predict(X_test_scaled)\n",
    "        y_pred_proba = self.model.predict_proba(X_test_scaled)[:, 1]\n",
    "        \n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1_score': f1_score(y_test, y_pred)\n",
    "        }\n",
    "        \n",
    "        cv_scores = cross_val_score(self.model, self.scaler.transform(X), y, cv=5)\n",
    "        metrics['cv_mean'] = cv_scores.mean()\n",
    "        metrics['cv_std'] = cv_scores.std()\n",
    "        \n",
    "        self.is_trained = True\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def predict(self, materials_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Predict synthesizability for new materials.\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model must be trained before making predictions\")\n",
    "        \n",
    "        X_pred = materials_df[self.feature_columns].copy()\n",
    "        X_pred = X_pred.fillna(X_pred.mean())\n",
    "        X_pred_scaled = self.scaler.transform(X_pred)\n",
    "        \n",
    "        predictions = self.model.predict(X_pred_scaled)\n",
    "        probabilities = self.model.predict_proba(X_pred_scaled)[:, 1]\n",
    "        \n",
    "        results_df = materials_df.copy()\n",
    "        results_df['synthesizability_prediction'] = predictions\n",
    "        results_df['synthesizability_probability'] = probabilities\n",
    "        results_df['synthesizability_confidence'] = np.abs(probabilities - 0.5) * 2\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    def predict_single(self, material_properties: Dict) -> Dict:\n",
    "        \"\"\"Predict synthesizability for a single material.\"\"\"\n",
    "        df = pd.DataFrame([material_properties])\n",
    "        result_df = self.predict(df)\n",
    "        \n",
    "        return {\n",
    "            'prediction': int(result_df['synthesizability_prediction'].iloc[0]),\n",
    "            'probability': float(result_df['synthesizability_probability'].iloc[0]),\n",
    "            'confidence': float(result_df['synthesizability_confidence'].iloc[0])\n",
    "        }\n",
    "\n",
    "class LLMSynthesizabilityPredictor:\n",
    "    \"\"\"LLM-based synthesizability predictor using rule-based heuristics.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.rules = {\n",
    "            'thermodynamic_stability': {\n",
    "                'energy_above_hull_threshold': 0.1,\n",
    "                'formation_energy_min': -4.0,\n",
    "                'formation_energy_max': 1.0\n",
    "            },\n",
    "            'structural_complexity': {\n",
    "                'nsites_max': 20,\n",
    "                'density_min': 2.0,\n",
    "                'density_max': 25.0\n",
    "            },\n",
    "            'electronic_properties': {\n",
    "                'band_gap_max': 8.0\n",
    "            },\n",
    "            'elemental_properties': {\n",
    "                'electronegativity_min': 0.7,\n",
    "                'electronegativity_max': 2.8,\n",
    "                'atomic_radius_min': 0.8,\n",
    "                'atomic_radius_max': 2.5\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def predict_synthesizability(self, material_data: Dict) -> Dict:\n",
    "        \"\"\"Predict synthesizability using rule-based system.\"\"\"\n",
    "        score = 0.0\n",
    "        max_score = 10.0\n",
    "        reasons = []\n",
    "        \n",
    "        e_hull = material_data.get('energy_above_hull', 0)\n",
    "        if e_hull <= self.rules['thermodynamic_stability']['energy_above_hull_threshold']:\n",
    "            score += 2.0\n",
    "            reasons.append(f\"Low energy above hull ({e_hull:.3f} eV/atom) indicates stability\")\n",
    "        else:\n",
    "            reasons.append(f\"High energy above hull ({e_hull:.3f} eV/atom) indicates instability\")\n",
    "        \n",
    "        formation_energy = material_data.get('formation_energy_per_atom', 0)\n",
    "        if (self.rules['thermodynamic_stability']['formation_energy_min'] <= \n",
    "            formation_energy <= self.rules['thermodynamic_stability']['formation_energy_max']):\n",
    "            score += 1.5\n",
    "            reasons.append(f\"Formation energy ({formation_energy:.3f} eV/atom) is reasonable\")\n",
    "        else:\n",
    "            reasons.append(f\"Formation energy ({formation_energy:.3f} eV/atom) is extreme\")\n",
    "        \n",
    "        nsites = material_data.get('nsites', 10)\n",
    "        if nsites <= self.rules['structural_complexity']['nsites_max']:\n",
    "            score += 1.0\n",
    "            reasons.append(f\"Unit cell size ({nsites} sites) is reasonable\")\n",
    "        else:\n",
    "            reasons.append(f\"Unit cell size ({nsites} sites) is very large\")\n",
    "        \n",
    "        density = material_data.get('density', 5.0)\n",
    "        if (self.rules['structural_complexity']['density_min'] <= density <= \n",
    "            self.rules['structural_complexity']['density_max']):\n",
    "            score += 1.0\n",
    "            reasons.append(f\"Density ({density:.1f} g/cm¬≥) is reasonable\")\n",
    "        else:\n",
    "            reasons.append(f\"Density ({density:.1f} g/cm¬≥) is unusual\")\n",
    "        \n",
    "        band_gap = material_data.get('band_gap', 0)\n",
    "        if band_gap <= self.rules['electronic_properties']['band_gap_max']:\n",
    "            score += 1.5\n",
    "            reasons.append(f\"Band gap ({band_gap:.1f} eV) is reasonable\")\n",
    "        else:\n",
    "            reasons.append(f\"Band gap ({band_gap:.1f} eV) is very wide\")\n",
    "        \n",
    "        electronegativity = material_data.get('electronegativity', 1.5)\n",
    "        if (self.rules['elemental_properties']['electronegativity_min'] <= \n",
    "            electronegativity <= self.rules['elemental_properties']['electronegativity_max']):\n",
    "            score += 1.0\n",
    "            reasons.append(f\"Electronegativity ({electronegativity:.2f}) is reasonable\")\n",
    "        else:\n",
    "            reasons.append(f\"Electronegativity ({electronegativity:.2f}) is extreme\")\n",
    "        \n",
    "        atomic_radius = material_data.get('atomic_radius', 1.4)\n",
    "        if (self.rules['elemental_properties']['atomic_radius_min'] <= \n",
    "            atomic_radius <= self.rules['elemental_properties']['atomic_radius_max']):\n",
    "            score += 1.0\n",
    "            reasons.append(f\"Atomic radius ({atomic_radius:.2f} √Ö) is reasonable\")\n",
    "        else:\n",
    "            reasons.append(f\"Atomic radius ({atomic_radius:.2f} √Ö) is unusual\")\n",
    "        \n",
    "        probability = 1 / (1 + np.exp(-(score - max_score/2) * 2))\n",
    "        prediction = 1 if probability >= 0.5 else 0\n",
    "        \n",
    "        return {\n",
    "            'prediction': prediction,\n",
    "            'probability': probability,\n",
    "            'confidence': min(probability, 1-probability) * 2,\n",
    "            'score': score,\n",
    "            'max_score': max_score,\n",
    "            'reasons': reasons\n",
    "        }\n",
    "\n",
    "def thermodynamic_stability_check(materials_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Check thermodynamic stability of materials.\"\"\"\n",
    "    results_df = materials_df.copy()\n",
    "    \n",
    "    results_df['thermodynamically_stable'] = results_df['energy_above_hull'] <= 0.1\n",
    "    results_df['formation_energy_reasonable'] = (\n",
    "        (results_df['formation_energy_per_atom'] >= -10) & \n",
    "        (results_df['formation_energy_per_atom'] <= 2)\n",
    "    )\n",
    "    results_df['overall_stability'] = (\n",
    "        results_df['thermodynamically_stable'] & \n",
    "        results_df['formation_energy_reasonable']\n",
    "    )\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def generate_precursor_recommendations(material_data: Dict) -> List[Dict]:\n",
    "    \"\"\"Generate synthesis precursor recommendations.\"\"\"\n",
    "    recommendations = []\n",
    "    \n",
    "    stability_score = 1 - material_data.get('energy_above_hull', 0) / 0.5\n",
    "    stability_score = np.clip(stability_score, 0, 1)\n",
    "    \n",
    "    if stability_score > 0.7:\n",
    "        recommendations.append({\n",
    "            'method': 'Solid State Reaction',\n",
    "            'precursors': ['Elemental powders', 'Binary oxides'],\n",
    "            'temperature_range': '800-1200¬∞C',\n",
    "            'difficulty': 'Medium',\n",
    "            'success_probability': 0.8\n",
    "        })\n",
    "    \n",
    "    if material_data.get('band_gap', 0) < 2.0:\n",
    "        recommendations.append({\n",
    "            'method': 'Arc Melting',\n",
    "            'precursors': ['Pure elements', 'Master alloy'],\n",
    "            'temperature_range': 'High temperature',\n",
    "            'difficulty': 'Low',\n",
    "            'success_probability': 0.9\n",
    "        })\n",
    "    \n",
    "    recommendations.append({\n",
    "        'method': 'Chemical Vapor Deposition',\n",
    "        'precursors': ['Metal halides', 'Hydrogen', 'Inert gas'],\n",
    "        'temperature_range': '600-1000¬∞C',\n",
    "        'difficulty': 'High',\n",
    "        'success_probability': 0.6\n",
    "    })\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "def calculate_synthesis_priority(materials_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Calculate priority ranking for experimental synthesis.\"\"\"\n",
    "    results_df = materials_df.copy()\n",
    "    \n",
    "    stability_score = 1 - results_df['energy_above_hull'].clip(0, 1)\n",
    "    synth_score = results_df.get('synthesizability_probability', 0.5)\n",
    "    novelty_score = results_df['energy_above_hull'].clip(0, 0.5) / 0.5\n",
    "    ease_score = 1 / (1 + results_df['nsites'] / 10)\n",
    "    \n",
    "    priority_score = (\n",
    "        0.4 * stability_score +\n",
    "        0.3 * synth_score +\n",
    "        0.2 * novelty_score +\n",
    "        0.1 * ease_score\n",
    "    )\n",
    "    \n",
    "    results_df['synthesis_priority_score'] = priority_score\n",
    "    results_df['synthesis_priority_rank'] = priority_score.rank(ascending=False).astype(int)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def cost_benefit_analysis(material_data: Dict) -> Dict:\n",
    "    \"\"\"Perform cost-benefit analysis for material synthesis.\"\"\"\n",
    "    base_costs = {\n",
    "        'solid_state': {'equipment': 500, 'materials': 200, 'time': 48, 'labor': 400},\n",
    "        'arc_melting': {'equipment': 200, 'materials': 150, 'time': 8, 'labor': 150},\n",
    "        'cvd': {'equipment': 2000, 'materials': 500, 'time': 72, 'labor': 800}\n",
    "    }\n",
    "    \n",
    "    stability = 1 - material_data.get('energy_above_hull', 0) / 0.5\n",
    "    stability = np.clip(stability, 0, 1)\n",
    "    synthesizability = material_data.get('synthesizability_probability', 0.5)\n",
    "    \n",
    "    if material_data.get('band_gap', 0) < 2.0:\n",
    "        method = 'arc_melting'\n",
    "        success_prob = min(0.9, stability * synthesizability * 1.2)\n",
    "    elif stability > 0.7:\n",
    "        method = 'solid_state'\n",
    "        success_prob = min(0.8, stability * synthesizability * 1.1)\n",
    "    else:\n",
    "        method = 'cvd'\n",
    "        success_prob = min(0.6, stability * synthesizability)\n",
    "    \n",
    "    costs = base_costs[method]\n",
    "    expected_cost = sum(costs.values()) / success_prob\n",
    "    \n",
    "    if material_data.get('band_gap', 0) > 1.5:\n",
    "        value = 10000 * stability * synthesizability\n",
    "    elif material_data.get('total_magnetization', 0) > 0.1:\n",
    "        value = 8000 * stability * synthesizability\n",
    "    else:\n",
    "        value = 5000 * stability * synthesizability\n",
    "    \n",
    "    expected_value = value * success_prob\n",
    "    net_benefit = expected_value - expected_cost\n",
    "    \n",
    "    return {\n",
    "        'recommended_method': method,\n",
    "        'success_probability': success_prob,\n",
    "        'total_cost': sum(costs.values()),\n",
    "        'expected_cost': expected_cost,\n",
    "        'expected_value': expected_value,\n",
    "        'net_benefit': net_benefit,\n",
    "        'benefit_cost_ratio': expected_value / expected_cost if expected_cost > 0 else 0\n",
    "    }\n",
    "\n",
    "def create_experimental_workflow(materials_df: pd.DataFrame, top_n: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"Create prioritized experimental validation workflow.\"\"\"\n",
    "    prioritized_df = calculate_synthesis_priority(materials_df)\n",
    "    workflow_df = prioritized_df.sort_values('synthesis_priority_score', ascending=False).head(top_n).copy()\n",
    "    \n",
    "    workflow_df['workflow_step'] = range(1, len(workflow_df) + 1)\n",
    "    workflow_df['batch_number'] = ((workflow_df['workflow_step'] - 1) // 5) + 1\n",
    "    workflow_df['estimated_time_days'] = np.random.uniform(3, 14, len(workflow_df)).round(1)\n",
    "    workflow_df['required_equipment'] = ['Arc Melter/Furnace'] * len(workflow_df)\n",
    "    workflow_df['priority_level'] = pd.cut(workflow_df['workflow_step'], \n",
    "                                         bins=[0, 3, 7, 10], labels=['High', 'Medium', 'Low'])\n",
    "    \n",
    "    return workflow_df[[\n",
    "        'workflow_step', 'batch_number', 'priority_level',\n",
    "        'synthesis_priority_score', 'synthesizability_probability',\n",
    "        'energy_above_hull', 'formation_energy_per_atom',\n",
    "        'estimated_time_days', 'required_equipment'\n",
    "    ]]\n",
    "\n",
    "# Initialize models\n",
    "def initialize_synthesizability_models():\n",
    "    \"\"\"Initialize and train synthesizability prediction models.\"\"\"\n",
    "    print(\"Training synthesizability prediction models...\")\n",
    "    \n",
    "    ml_classifier = SynthesizabilityClassifier(model_type='random_forest')\n",
    "    metrics = ml_classifier.train()\n",
    "    \n",
    "    print(\"ML Classifier trained successfully!\")\n",
    "    print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"  F1-Score: {metrics['f1_score']:.4f}\")\n",
    "    print(f\"  CV Mean: {metrics['cv_mean']:.4f} (+/- {metrics['cv_std']*2:.4f})\")\n",
    "    \n",
    "    llm_predictor = LLMSynthesizabilityPredictor()\n",
    "    \n",
    "    return ml_classifier, llm_predictor\n",
    "\n",
    "print(\"Synthesizability prediction module loaded!\")\n",
    "print(\"Initializing synthesizability prediction models...\")\n",
    "ml_classifier, llm_predictor = initialize_synthesizability_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "import-libraries",
    "outputId": "f15eb20b-72b3-40d7-e6a7-35f6945ad624"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from scipy.stats import ks_2samp\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Running on: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data-source-selection"
   },
   "source": [
    "## üÜï NEW: Choose Your Data Source\n",
    "\n",
    "This workshop now supports two data sources:\n",
    "\n",
    "1. **Synthetic Data** (Original): Programmatically generated for demonstrations\n",
    "2. **Materials Project Data** (NEW): Real materials from computational database\n",
    "\n",
    "Choose your data source below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118,
     "referenced_widgets": [
      "95a28574cbdd49d38d6a4913a76c570c",
      "bf83cc9f9e8c4fdb8a12c3a2080c3a83",
      "82e7dee5172b4d29b282271f79e47621"
     ]
    },
    "id": "data-source-choice",
    "outputId": "162f148f-077c-41b7-92fd-3f6f9716a065"
   },
   "outputs": [],
   "source": [
    "# Data source selection\n",
    "data_source = widgets.Dropdown(\n",
    "    options=['Synthetic (Demo)', 'Materials Project (Real)'],\n",
    "    value='Materials Project (Real)',\n",
    "    description='Data Source:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "display(data_source)\n",
    "\n",
    "print(\"\\nüéØ Selected data source will be loaded in the next cell.\")\n",
    "print(\"   - Synthetic: Fast, good for learning concepts\")\n",
    "print(\"   - Materials Project: Real data, production-ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "load-materials-data",
    "outputId": "92cff963-15d9-41f7-9146-7d850fab7cca"
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "# Materials Project API Integration Class\n",
    "class MaterialsProjectClient:\n",
    "    \"\"\"Client for Materials Project API with rate limiting and error handling.\"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str = None):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"https://api.materialsproject.org\"\n",
    "        self.last_request_time = 0\n",
    "        self.rate_limit_delay = 0.2\n",
    "        self.max_retries = 3\n",
    "\n",
    "    def _rate_limit_wait(self):\n",
    "        current_time = time.time()\n",
    "        time_since_last = current_time - self.last_request_time\n",
    "        if time_since_last < self.rate_limit_delay:\n",
    "            time.sleep(self.rate_limit_delay - time_since_last)\n",
    "        self.last_request_time = time.time()\n",
    "\n",
    "    def _make_request(self, endpoint: str, params: Dict = None) -> Dict:\n",
    "        if params is None:\n",
    "            params = {}\n",
    "        headers = {\"X-API-Key\": self.api_key}\n",
    "\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                self._rate_limit_wait()\n",
    "                response = requests.get(f\"{self.base_url}{endpoint}\", params=params, headers=headers, timeout=30)\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    return response.json()\n",
    "                elif response.status_code == 429:\n",
    "                    time.sleep(5)\n",
    "                    continue\n",
    "                else:\n",
    "                    if attempt < self.max_retries - 1:\n",
    "                        time.sleep(2 ** attempt)\n",
    "                        continue\n",
    "                    raise Exception(f\"API error {response.status_code}\")\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                if attempt < self.max_retries - 1:\n",
    "                    time.sleep(2 ** attempt)\n",
    "                    continue\n",
    "                raise\n",
    "\n",
    "        raise Exception(\"All API attempts failed\")\n",
    "\n",
    "    def get_materials_summary(self, elements: List[str] = None, limit: int = 100) -> pd.DataFrame:\n",
    "        params = {\n",
    "            \"_fields\": \"material_id,formula_pretty,elements,nsites,volume,density,density_atomic,band_gap,energy_above_hull,formation_energy_per_atom,total_magnetization\",\n",
    "            \"_limit\": limit\n",
    "        }\n",
    "\n",
    "        if elements:\n",
    "            params[\"elements\"] = \",\".join(elements)\n",
    "\n",
    "        response = self._make_request(\"/materials/summary/\", params)\n",
    "        materials = response.get(\"data\", [])\n",
    "\n",
    "        if not materials:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        df = pd.DataFrame(materials)\n",
    "        df.rename(columns={'formula_pretty': 'formula'}, inplace=True)\n",
    "\n",
    "        numeric_cols = ['nsites', 'volume', 'density', 'atomic_density', 'band_gap', 'energy_above_hull', 'formation_energy_per_atom', 'total_magnetization']\n",
    "        for col in numeric_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "        return df\n",
    "\n",
    "    def get_binary_alloys(self, element_pairs: List[Tuple[str, str]] = None, limit_per_pair: int = 50) -> pd.DataFrame:\n",
    "        if element_pairs is None:\n",
    "            element_pairs = [('Al', 'Ti'), ('Al', 'V'), ('Al', 'Cr'), ('Al', 'Fe'), ('Al', 'Ni'), ('Al', 'Cu'),\n",
    "                            ('Ti', 'V'), ('Ti', 'Cr'), ('Ti', 'Fe'), ('Ti', 'Ni'), ('V', 'Cr'), ('Fe', 'Co'), ('Fe', 'Ni'), ('Co', 'Ni'), ('Ni', 'Cu')]\n",
    "\n",
    "        all_materials = []\n",
    "        for elem1, elem2 in element_pairs:\n",
    "            materials = self.get_materials_summary(elements=[elem1, elem2], limit=limit_per_pair)\n",
    "            if not materials.empty:\n",
    "                materials['element_1'] = elem1\n",
    "                materials['element_2'] = elem2\n",
    "                materials['alloy_type'] = 'binary'\n",
    "                all_materials.append(materials)\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        if not all_materials:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        combined_df = pd.concat(all_materials, ignore_index=True)\n",
    "        combined_df.drop_duplicates(subset='material_id', inplace=True)\n",
    "        return combined_df\n",
    "\n",
    "# Load selected data source\n",
    "if data_source.value == 'Materials Project (Real)':\n",
    "    print(\"üîÑ Loading REAL materials data from Materials Project...\")\n",
    "\n",
    "    try:\n",
    "        client = MaterialsProjectClient()\n",
    "\n",
    "        # Test connection\n",
    "        test_data = client.get_materials_summary(elements=[\"Al\", \"Ti\"], limit=5)\n",
    "        if test_data.empty:\n",
    "            raise Exception(\"API connection failed\")\n",
    "\n",
    "        # Get full dataset\n",
    "        raw_data = client.get_binary_alloys(limit_per_pair=30)\n",
    "\n",
    "        if raw_data.empty:\n",
    "            raise Exception(\"No materials retrieved\")\n",
    "\n",
    "        # Convert to ML features\n",
    "        import pymatgen.core as mg\n",
    "\n",
    "        features_df = raw_data.copy()\n",
    "        for idx, row in features_df.iterrows():\n",
    "            if 'elements' in row and row['elements']:\n",
    "                elements = row['elements']\n",
    "                electronegativities = []\n",
    "                atomic_radii = []\n",
    "\n",
    "                for elem_symbol in elements:\n",
    "                    try:\n",
    "                        elem = mg.Element(elem_symbol)\n",
    "                        if hasattr(elem, 'X') and elem.X is not None:\n",
    "                            electronegativities.append(elem.X)\n",
    "                        if hasattr(elem, 'atomic_radius') and elem.atomic_radius is not None:\n",
    "                            atomic_radii.append(elem.atomic_radius)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                features_df.loc[idx, 'electronegativity'] = np.mean(electronegativities) if electronegativities else 0\n",
    "                features_df.loc[idx, 'atomic_radius'] = np.mean(atomic_radii) if atomic_radii else 0\n",
    "\n",
    "        features_df['composition_1'] = 0.5\n",
    "        features_df['composition_2'] = 0.5\n",
    "        features_df['composition_3'] = 0.0\n",
    "\n",
    "        ml_features = features_df[['composition_1', 'composition_2', 'composition_3', 'density', 'electronegativity', 'atomic_radius', 'band_gap', 'energy_above_hull', 'formation_energy_per_atom']].copy()\n",
    "        ml_features.rename(columns={'formation_energy_per_atom': 'melting_point'}, inplace=True)\n",
    "        ml_features.fillna(ml_features.mean(), inplace=True)\n",
    "\n",
    "        ml_features['melting_point'] = ml_features['melting_point'].clip(-10, 10)\n",
    "        ml_features['density'] = ml_features['density'].clip(0, 50)\n",
    "\n",
    "        data = features_df\n",
    "        data_type = \"real\"\n",
    "\n",
    "        print(f\"‚úÖ Loaded {len(ml_features)} REAL materials from Materials Project!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load Materials Project data: {e}\")\n",
    "        print(\"Falling back to synthetic data...\")\n",
    "        data_source.value = 'Synthetic (Demo)'\n",
    "\n",
    "if data_source.value == 'Synthetic (Demo)':\n",
    "    print(\"üîÑ Creating SYNTHETIC materials dataset for demonstration...\")\n",
    "\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "\n",
    "    alloys = []\n",
    "    elements = ['Al', 'Ti', 'V', 'Cr', 'Mn', 'Fe', 'Co', 'Ni', 'Cu', 'Zn']\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        alloy_type = np.random.choice(['binary', 'ternary'], p=[0.7, 0.3])\n",
    "\n",
    "        if alloy_type == 'binary':\n",
    "            elem1, elem2 = np.random.choice(elements, 2, replace=False)\n",
    "            comp1 = np.random.uniform(0.1, 0.9)\n",
    "            comp2 = 1 - comp1\n",
    "            comp3 = 0\n",
    "        else:\n",
    "            elem1, elem2, elem3 = np.random.choice(elements, 3, replace=False)\n",
    "            comp1 = np.random.uniform(0.1, 0.6)\n",
    "            comp2 = np.random.uniform(0.1, 0.6)\n",
    "            comp3 = 1 - comp1 - comp2\n",
    "\n",
    "        melting_point = np.random.normal(1500, 300)\n",
    "        density = np.random.normal(7.8, 2.0)\n",
    "        electronegativity = np.random.normal(1.8, 0.3)\n",
    "        atomic_radius = np.random.normal(1.3, 0.2)\n",
    "\n",
    "        alloys.append({\n",
    "            'id': f'alloy_{i+1}',\n",
    "            'alloy_type': alloy_type,\n",
    "            'element_1': elem1,\n",
    "            'element_2': elem2,\n",
    "            'element_3': elem3 if alloy_type == 'ternary' else None,\n",
    "            'composition_1': comp1,\n",
    "            'composition_2': comp2,\n",
    "            'composition_3': comp3,\n",
    "            'melting_point': max(500, melting_point),\n",
    "            'density': max(2, density),\n",
    "            'electronegativity': max(0.7, min(2.5, electronegativity)),\n",
    "            'atomic_radius': max(1.0, min(1.8, atomic_radius))\n",
    "        })\n",
    "\n",
    "    data = pd.DataFrame(alloys)\n",
    "    data_type = \"synthetic\"\n",
    "\n",
    "    # Create ML features from synthetic data\n",
    "    binary_data_synth = data[data['alloy_type'] == 'binary'].copy()\n",
    "    binary_data_synth['composition_3'] = binary_data_synth['composition_3'].fillna(0)\n",
    "    ml_features = binary_data_synth[['composition_1', 'composition_2', 'composition_3', 'melting_point', 'density', 'electronegativity', 'atomic_radius']].copy()\n",
    "    ml_features['band_gap'] = 0.0\n",
    "    ml_features['energy_above_hull'] = 0.0\n",
    "\n",
    "    print(f\"‚úÖ Created {len(ml_features)} SYNTHETIC materials for demonstration!\")\n",
    "\n",
    "print(f\"\\nüìä Dataset ready: {len(ml_features)} materials ({data_type} data)\")\n",
    "print(\"First few rows:\")\n",
    "display_cols = ['alloy_type', 'element_1', 'element_2', 'density', 'melting_point'] if data_type == 'synthetic' else ['formula', 'elements', 'density', 'band_gap']\n",
    "print(data[display_cols].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "explore-dataset",
    "outputId": "3c5570a7-d8c7-4725-b386-ac6050d0c740"
   },
   "outputs": [],
   "source": [
    "# Explore the loaded dataset\n",
    "print(\"üìä DATASET EXPLORATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if data_type == 'real':\n",
    "    print(\"Alloy types distribution:\")\n",
    "    print(data['alloy_type'].value_counts())\n",
    "    print(\"\\nProperty statistics:\")\n",
    "    print(data[['density', 'band_gap', 'energy_above_hull']].describe())\n",
    "\n",
    "    # Show unique element combinations\n",
    "    element_pairs = data.apply(lambda x: f\"{x['element_1']}-{x['element_2']}\", axis=1)\n",
    "    print(\"\\nTop element combinations:\")\n",
    "    print(element_pairs.value_counts().head(10))\n",
    "\n",
    "else:\n",
    "    print(\"Alloy types distribution:\")\n",
    "    print(data['alloy_type'].value_counts())\n",
    "    print(\"\\nProperty statistics:\")\n",
    "    print(data[['melting_point', 'density', 'electronegativity', 'atomic_radius']].describe())\n",
    "\n",
    "# Data quality check\n",
    "missing_values = ml_features.isnull().sum().sum()\n",
    "print(f\"\\nMissing values in dataset: {missing_values}\")\n",
    "print(f\"Data shape: {ml_features.shape}\")\n",
    "print(f\"Features: {list(ml_features.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "interactive-parameters"
   },
   "source": [
    "## Interactive Parameters\n",
    "\n",
    "Let's set up some interactive controls to experiment with different model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161,
     "referenced_widgets": [
      "9c3a87c534dd4a418eeabbc8a44e79ef",
      "64f631411f914846b5bee50534fc99fa",
      "deee3fc3be734f59858a73d7618956f2",
      "e65b68b7cba8424eab5e785096e0f8e6",
      "6648ed737ab349fdac1455faa02df382",
      "6417d7434dd44b0c9a9410d71d2d3078",
      "bb0dd304b5a1426fabbc0989ab819193",
      "945d3ada9c7843a2935c2a91cf3494f5",
      "76ebce82bb0446c4b0552250bf2806b5"
     ]
    },
    "id": "parameter-controls",
    "outputId": "fdb52aa8-e2df-4ee3-dcc5-4ca462b3d809"
   },
   "outputs": [],
   "source": [
    "# Interactive parameter controls\n",
    "latent_dim_slider = widgets.IntSlider(value=5, min=2, max=20, step=1, description='Latent Dim:')\n",
    "epochs_slider = widgets.IntSlider(value=50, min=10, max=200, step=10, description='Epochs:')\n",
    "num_samples_slider = widgets.IntSlider(value=100, min=10, max=500, step=10, description='Samples:')\n",
    "\n",
    "display(latent_dim_slider, epochs_slider, num_samples_slider)\n",
    "\n",
    "# Global parameters (will be updated by widgets)\n",
    "params = {\n",
    "    'latent_dim': latent_dim_slider.value,\n",
    "    'epochs': epochs_slider.value,\n",
    "    'num_samples': num_samples_slider.value\n",
    "}\n",
    "\n",
    "def update_params(change):\n",
    "    params['latent_dim'] = latent_dim_slider.value\n",
    "    params['epochs'] = epochs_slider.value\n",
    "    params['num_samples'] = num_samples_slider.value\n",
    "    print(f\"Updated parameters: {params}\")\n",
    "\n",
    "latent_dim_slider.observe(update_params, names='value')\n",
    "epochs_slider.observe(update_params, names='value')\n",
    "num_samples_slider.observe(update_params, names='value')\n",
    "\n",
    "print(\"Interactive controls ready! Adjust the sliders and rerun cells below.\")\n",
    "print(f\"\\nüéØ Training on {data_type.upper()} data with {len(ml_features)} materials!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data-preprocessing"
   },
   "source": [
    "## Step 2: Data Preprocessing\n",
    "\n",
    "We need to prepare our data for machine learning. This involves:\n",
    "- Selecting relevant features\n",
    "- Handling missing values\n",
    "- Scaling the data\n",
    "\n",
    "Let's focus on binary alloys for this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "preprocess-data",
    "outputId": "0af4f587-28d7-41ad-8c7e-4ae73ae3cf47"
   },
   "outputs": [],
   "source": [
    "# Select features and prepare for ML\n",
    "feature_cols = ['composition_1', 'composition_2', 'melting_point', 'density', 'electronegativity', 'atomic_radius']\n",
    "features = ml_features[feature_cols].values\n",
    "\n",
    "print(f\"Using {len(ml_features)} materials\")\n",
    "print(f\"Feature matrix shape: {features.shape}\")\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "print(\"Features scaled successfully!\")\n",
    "\n",
    "# Show feature statistics\n",
    "print(\"\\nFeature scaling statistics:\")\n",
    "scaled_df = pd.DataFrame(features_scaled, columns=feature_cols)\n",
    "print(scaled_df.describe().loc[['mean', 'std']].round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vae-architecture"
   },
   "source": [
    "## Step 3: The Variational Autoencoder (VAE)\n",
    "\n",
    "A VAE is a type of neural network that can learn to generate new data similar to its training data. Here's how it works:\n",
    "\n",
    "- **Encoder**: Compresses input data into a lower-dimensional latent space\n",
    "- **Latent Space**: A compressed representation where similar materials are close together\n",
    "- **Decoder**: Reconstructs data from the latent space\n",
    "\n",
    "The \"variational\" part means it learns a probability distribution, allowing us to sample new materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "define-vae",
    "outputId": "fbfe21a0-c3d2-495b-fe80-a98e9b85e7d3"
   },
   "outputs": [],
   "source": [
    "class OptimizedVAE(nn.Module):\n",
    "    \"\"\"Optimized Variational Autoencoder for materials discovery with improved convergence.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int = 6, latent_dim: int = 5):\n",
    "        super(OptimizedVAE, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder - increased capacity for better convergence\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(32, latent_dim)\n",
    "        self.fc_var = nn.Linear(32, latent_dim)\n",
    "\n",
    "        # Decoder - symmetric to encoder, no sigmoid for unbounded features\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_dim)\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        log_var = self.fc_var(h)\n",
    "        return mu, log_var\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        reconstructed = self.decode(z)\n",
    "        return reconstructed, mu, log_var\n",
    "\n",
    "print(\"VAE class defined successfully!\")\n",
    "print(f\"\\nüéØ Ready to train on {data_type.upper()} data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training-section"
   },
   "source": [
    "## Step 4: Training the VAE\n",
    "\n",
    "Now let's train our VAE on the selected dataset. The model will learn to compress and reconstruct materials data, enabling generation of new materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "train-vae",
    "outputId": "4e6bee02-d761-48c5-8b15-caf5bd7b53dc"
   },
   "outputs": [],
   "source": [
    "# Initialize and train the optimized VAE\n",
    "try:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    input_dim = features_scaled.shape[1]\n",
    "    model = OptimizedVAE(input_dim=input_dim, latent_dim=params['latent_dim']).to(device)\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    features_tensor = torch.FloatTensor(features_scaled)\n",
    "    dataset = torch.utils.data.TensorDataset(features_tensor, features_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # Training setup\n",
    "    initial_lr = 0.005\n",
    "    optimizer = optim.Adam(model.parameters(), lr=initial_lr)\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.995)\n",
    "    epochs = params['epochs']\n",
    "\n",
    "    print(f\"üöÄ Training optimized VAE on {data_type.upper()} data for {epochs} epochs...\")\n",
    "    print(f\"üìä Dataset: {len(ml_features)} materials, {input_dim} features\")\n",
    "    print(f\"üß† Model: {input_dim} ‚Üí 64 ‚Üí 32 ‚Üí {model.latent_dim} ‚Üí 32 ‚Üí 64 ‚Üí {input_dim}\")\n",
    "    print(f\"‚ö° Running on: {device}\")\n",
    "    print(\"\\nThis may take a minute or two...\")\n",
    "\n",
    "    model.train()\n",
    "    losses = []\n",
    "    reconstruction_losses = []\n",
    "    kl_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        epoch_recon_loss = 0\n",
    "        epoch_kl_loss = 0\n",
    "\n",
    "        for batch_x, _ in dataloader:\n",
    "            batch_x = batch_x.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            reconstructed, mu, log_var = model(batch_x)\n",
    "\n",
    "            # Compute losses\n",
    "            reconstruction_loss = nn.functional.mse_loss(reconstructed, batch_x, reduction='sum')\n",
    "            kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "\n",
    "            kl_weight = min(1.0, epoch / 10.0)\n",
    "            loss = reconstruction_loss + kl_weight * kl_loss\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_recon_loss += reconstruction_loss.item()\n",
    "            epoch_kl_loss += kl_loss.item()\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        avg_recon_loss = epoch_recon_loss / len(dataloader)\n",
    "        avg_kl_loss = epoch_kl_loss / len(dataloader)\n",
    "\n",
    "        losses.append(avg_loss)\n",
    "        reconstruction_losses.append(avg_recon_loss)\n",
    "        kl_losses.append(avg_kl_loss)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Total Loss: {avg_loss:.4f}, Recon: {avg_recon_loss:.4f}, KL: {avg_kl_loss:.4f}\")\n",
    "\n",
    "    print(f\"\\n‚úÖ VAE training completed on {data_type.upper()} data!\")\n",
    "    print(f\"üìà Final loss: {losses[-1]:.4f}\")\n",
    "    print(f\"üéØ Trained on {len(ml_features)} {data_type} materials\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training error: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "generation-section"
   },
   "source": [
    "## Step 5: Generating New Materials\n",
    "\n",
    "Now that we have a trained VAE, we can generate new materials by sampling from the latent space. This is like asking the model to \"imagine\" new alloys that follow the patterns it learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "generate-new-materials",
    "outputId": "1eddc7ec-68c2-459a-c077-91894f4a63fb"
   },
   "outputs": [],
   "source": [
    "# Generate new materials\n",
    "model.eval()\n",
    "num_samples = params['num_samples']\n",
    "\n",
    "print(f\"üé® Generating {num_samples} new material compositions...\")\n",
    "print(f\"üìö Based on patterns learned from {data_type.upper()} data\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Sample from latent space\n",
    "    z = torch.randn(num_samples, model.latent_dim).to(device)\n",
    "    generated_features = model.decode(z).cpu().numpy()\n",
    "\n",
    "    # Inverse transform to original scale\n",
    "    generated_features = scaler.inverse_transform(generated_features)\n",
    "\n",
    "# Create DataFrame with generated materials\n",
    "elements = ['Al', 'Ti', 'V', 'Cr', 'Mn', 'Fe', 'Co', 'Ni', 'Cu', 'Zn']\n",
    "new_materials = []\n",
    "\n",
    "for i, features in enumerate(generated_features):\n",
    "    elem1, elem2 = random.sample(elements, 2)\n",
    "    comp1 = max(0.1, min(0.9, features[0]))\n",
    "    comp2 = 1.0 - comp1\n",
    "\n",
    "    material = {\n",
    "        'id': f'generated_{i+1}',\n",
    "        'element_1': elem1,\n",
    "        'element_2': elem2,\n",
    "        'composition_1': comp1,\n",
    "        'composition_2': comp2,\n",
    "        'formula': f'{elem1}{comp1:.3f}{elem2}{comp2:.3f}',\n",
    "        'melting_point': abs(features[2]),\n",
    "        'density': abs(features[3]),\n",
    "        'electronegativity': max(0, features[4]),\n",
    "        'atomic_radius': max(0, features[5]),\n",
    "        'data_source': data_type,\n",
    "        'is_generated': True\n",
    "    }\n",
    "    new_materials.append(material)\n",
    "\n",
    "generated_df = pd.DataFrame(new_materials)\n",
    "print(f\"‚úÖ Generated {len(generated_df)} new materials!\")\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\nüß™ Example generated materials:\")\n",
    "display_cols = ['formula', 'melting_point', 'density']\n",
    "print(generated_df[display_cols].head(10))\n",
    "\n",
    "if data_type == 'real':\n",
    "    print(\"\\nüéØ These materials are generated based on REAL Materials Project data!\")\n",
    "    print(\"üî¨ They could potentially be synthesized and tested experimentally.\")\n",
    "else:\n",
    "    print(\"\\nüìö These materials are generated based on SYNTHETIC data patterns.\")\n",
    "    print(\"üß™ Great for learning ML concepts and testing workflows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "initialize-synthesizability-models"
   },
   "outputs": [],
   "source": [
    "# Synthesizability models are already initialized in the first cell above\n",
    "print(\"Using pre-initialized synthesizability models...\")\n",
    "print(f\"ML Classifier trained: {ml_classifier.is_trained}\")\n",
    "print(f\"LLM Predictor ready: {hasattr(llm_predictor, 'rules')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "thermodynamic-validation"
   },
   "outputs": [],
   "source": [
    "# First, validate thermodynamic stability of generated materials\n",
    "print(\"=== THERMODYNAMIC STABILITY VALIDATION ===\")\n",
    "\n",
    "# Prepare generated materials data for validation\n",
    "generated_for_validation = generated_df.copy()\n",
    "\n",
    "# Add required columns for validation\n",
    "generated_for_validation['formation_energy_per_atom'] = generated_for_validation['melting_point']  # Approximation\n",
    "generated_for_validation['energy_above_hull'] = np.random.uniform(0, 0.5, len(generated_for_validation))  # Simulated\n",
    "generated_for_validation['band_gap'] = np.random.uniform(0, 3, len(generated_for_validation))  # Simulated\n",
    "generated_for_validation['electronegativity'] = generated_for_validation['electronegativity']\n",
    "generated_for_validation['atomic_radius'] = generated_for_validation['atomic_radius']\n",
    "generated_for_validation['nsites'] = np.random.randint(2, 8, len(generated_for_validation))  # Simulated\n",
    "\n",
    "# Perform thermodynamic stability check\n",
    "stability_results = thermodynamic_stability_check(generated_for_validation)\n",
    "\n",
    "print(f\"Generated materials stability analysis:\")\n",
    "print(f\"- Thermodynamically stable (E_hull < 0.1): {stability_results['thermodynamically_stable'].sum()}/{len(stability_results)} ({stability_results['thermodynamically_stable'].mean()*100:.1f}%)\")\n",
    "print(f\"- Reasonable formation energies: {stability_results['formation_energy_reasonable'].sum()}/{len(stability_results)} ({stability_results['formation_energy_reasonable'].mean()*100:.1f}%)\")\n",
    "print(f\"- Overall stable materials: {stability_results['overall_stability'].sum()}/{len(stability_results)} ({stability_results['overall_stability'].mean()*100:.1f}%)\")\n",
    "\n",
    "# Filter to stable materials only\n",
    "stable_materials = stability_results[stability_results['overall_stability']].copy()\n",
    "print(f\"\\nProceeding with {len(stable_materials)} thermodynamically stable materials for synthesizability analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "synthesizability-prediction"
   },
   "outputs": [],
   "source": [
    "# Predict synthesizability using ML and LLM approaches\n",
    "print(\"\\n=== SYNTHESIZABILITY PREDICTION ===\")\n",
    "\n",
    "# ML-based prediction\n",
    "print(\"Running ML-based synthesizability classifier...\")\n",
    "ml_predictions = ml_classifier.predict(stable_materials)\n",
    "\n",
    "# LLM-based prediction (rule-based system)\n",
    "print(\"Running LLM-based synthesizability prediction...\")\n",
    "llm_predictions = []\n",
    "for idx, material in stable_materials.iterrows():\n",
    "    llm_result = llm_predictor.predict_synthesizability(material.to_dict())\n",
    "    llm_predictions.append(llm_result)\n",
    "\n",
    "# Combine predictions\n",
    "synthesis_results = ml_predictions.copy()\n",
    "synthesis_results['llm_prediction'] = [p['prediction'] for p in llm_predictions]\n",
    "synthesis_results['llm_probability'] = [p['probability'] for p in llm_predictions]\n",
    "synthesis_results['llm_confidence'] = [p['confidence'] for p in llm_predictions]\n",
    "synthesis_results['llm_score'] = [p['score'] for p in llm_predictions]\n",
    "\n",
    "# Ensemble prediction (weighted average)\n",
    "synthesis_results['ensemble_probability'] = (\n",
    "    0.7 * synthesis_results['synthesizability_probability'] +\n",
    "    0.3 * synthesis_results['llm_probability']\n",
    ")\n",
    "synthesis_results['ensemble_prediction'] = (synthesis_results['ensemble_probability'] >= 0.5).astype(int)\n",
    "synthesis_results['ensemble_confidence'] = np.abs(synthesis_results['ensemble_probability'] - 0.5) * 2\n",
    "\n",
    "print(\"\\nSynthesizability prediction results:\")\n",
    "print(f\"- ML Classifier accuracy: {ml_classifier.is_trained}\")\n",
    "print(f\"- Materials predicted synthesizable by ML: {synthesis_results['synthesizability_prediction'].sum()}/{len(synthesis_results)} ({synthesis_results['synthesizability_prediction'].mean()*100:.1f}%)\")\n",
    "print(f\"- Materials predicted synthesizable by LLM: {synthesis_results['llm_prediction'].sum()}/{len(synthesis_results)} ({synthesis_results['llm_prediction'].mean()*100:.1f}%)\")\n",
    "print(f\"- Materials predicted synthesizable by Ensemble: {synthesis_results['ensemble_prediction'].sum()}/{len(synthesis_results)} ({synthesis_results['ensemble_prediction'].mean()*100:.1f}%)\")\n",
    "\n",
    "# High-confidence predictions\n",
    "high_conf_ml = synthesis_results[synthesis_results['synthesizability_confidence'] > 0.8]\n",
    "high_conf_llm = synthesis_results[synthesis_results['llm_confidence'] > 0.8]\n",
    "high_conf_ensemble = synthesis_results[synthesis_results['ensemble_confidence'] > 0.8]\n",
    "\n",
    "print(f\"\\nHigh-confidence predictions (>80% confidence):\")\n",
    "print(f\"- ML: {len(high_conf_ml)} materials\")\n",
    "print(f\"- LLM: {len(high_conf_llm)} materials\")\n",
    "print(f\"- Ensemble: {len(high_conf_ensemble)} materials\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "synthesis-priority-ranking"
   },
   "outputs": [],
   "source": [
    "# Calculate synthesis priority ranking\n",
    "print(\"\\n=== SYNTHESIS PRIORITY RANKING ===\")\n",
    "\n",
    "# Calculate priorities for all stable materials\n",
    "priority_results = calculate_synthesis_priority(synthesis_results)\n",
    "\n",
    "# Show top 10 materials by synthesis priority\n",
    "top_candidates = priority_results.head(10)[[\n",
    "    'formula', 'synthesis_priority_score', 'synthesis_priority_rank',\n",
    "    'ensemble_probability', 'ensemble_confidence', 'energy_above_hull',\n",
    "    'density', 'melting_point'\n",
    "]]\n",
    "\n",
    "print(\"Top 10 synthesis candidates:\")\n",
    "print(top_candidates.to_string(index=False))\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nPriority score statistics:\")\n",
    "print(f\"- Mean priority score: {priority_results['synthesis_priority_score'].mean():.3f}\")\n",
    "print(f\"- Max priority score: {priority_results['synthesis_priority_score'].max():.3f}\")\n",
    "print(f\"- Materials with priority > 0.7: {(priority_results['synthesis_priority_score'] > 0.7).sum()}\")\n",
    "\n",
    "# Distribution analysis\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(priority_results['synthesis_priority_score'], bins=20, alpha=0.7, color='green')\n",
    "plt.xlabel('Synthesis Priority Score')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Priority Score Distribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(priority_results['energy_above_hull'], priority_results['synthesis_priority_score'],\n",
    "           alpha=0.6, c=priority_results['ensemble_probability'], cmap='RdYlGn')\n",
    "plt.xlabel('Energy Above Hull')\n",
    "plt.ylabel('Priority Score')\n",
    "plt.title('Stability vs Priority')\n",
    "plt.colorbar(label='Synth. Probability')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(priority_results['ensemble_probability'], priority_results['synthesis_priority_score'],\n",
    "           alpha=0.6, c=priority_results['ensemble_confidence'], cmap='Blues')\n",
    "plt.xlabel('Synthesizability Probability')\n",
    "plt.ylabel('Priority Score')\n",
    "plt.title('Synth. Prob. vs Priority')\n",
    "plt.colorbar(label='Confidence')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('synthesis_priority_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "precursor-recommendations"
   },
   "outputs": [],
   "source": [
    "# Generate synthesis precursor recommendations\n",
    "print(\"\\n=== SYNTHESIS PRECURSOR RECOMMENDATIONS ===\")\n",
    "\n",
    "# Get recommendations for top 5 candidates\n",
    "top_5_candidates = priority_results.head(5)\n",
    "\n",
    "print(\"Synthesis recommendations for top 5 candidates:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, (_, candidate) in enumerate(top_5_candidates.iterrows(), 1):\n",
    "    print(f\"\\nCandidate {i}: {candidate['formula']}\")\n",
    "    print(f\"Priority Score: {candidate['synthesis_priority_score']:.3f}\")\n",
    "    print(f\"Synthesizability: {candidate['ensemble_probability']:.3f}\")\n",
    "    print(f\"Stability (E_hull): {candidate['energy_above_hull']:.3f}\")\n",
    "    \n",
    "    # Get precursor recommendations\n",
    "    precursors = generate_precursor_recommendations(candidate.to_dict())\n",
    "    \n",
    "    print(\"Recommended synthesis methods:\")\n",
    "    for j, method in enumerate(precursors, 1):\n",
    "        print(f\"  {j}. {method['method']} ({method['difficulty']})\")\n",
    "        print(f\"     Precursors: {', '.join(method['precursors'])}\")\n",
    "        print(f\"     Conditions: {method['temperature_range']}\")\n",
    "        print(f\"     Success probability: {method['success_probability']:.1f}\")\n",
    "        if j < len(precursors):\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "cost-benefit-analysis"
   },
   "outputs": [],
   "source": [
    "# Perform cost-benefit analysis\n",
    "print(\"\\n=== COST-BENEFIT ANALYSIS ===\")\n",
    "\n",
    "# Analyze costs and benefits for top candidates\n",
    "cba_results = []\n",
    "\n",
    "for _, candidate in top_5_candidates.iterrows():\n",
    "    cba = cost_benefit_analysis(candidate.to_dict())\n",
    "    cba['formula'] = candidate['formula']\n",
    "    cba['priority_score'] = candidate['synthesis_priority_score']\n",
    "    cba_results.append(cba)\n",
    "\n",
    "# Display results in a table\n",
    "cba_df = pd.DataFrame(cba_results)\n",
    "cba_display = cba_df[['formula', 'recommended_method', 'success_probability', \n",
    "                      'total_cost', 'expected_cost', 'expected_value', 'net_benefit', 'benefit_cost_ratio']].copy()\n",
    "\n",
    "print(\"Cost-benefit analysis for top 5 candidates:\")\n",
    "print(cba_display.to_string(index=False, float_format='%.2f'))\n",
    "\n",
    "# Summary statistics\n",
    "profitable_candidates = cba_df[cba_df['net_benefit'] > 0]\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"- Candidates with positive net benefit: {len(profitable_candidates)}/{len(cba_df)}\")\n",
    "print(f\"- Average expected cost: ${cba_df['expected_cost'].mean():.0f}\")\n",
    "print(f\"- Average expected value: ${cba_df['expected_value'].mean():.0f}\")\n",
    "print(f\"- Average net benefit: ${cba_df['net_benefit'].mean():.0f}\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(range(len(cba_df)), cba_df['net_benefit'], \n",
    "               color=['green' if x > 0 else 'red' for x in cba_df['net_benefit']])\n",
    "plt.xlabel('Candidate')\n",
    "plt.ylabel('Net Benefit ($)')\n",
    "plt.title('Cost-Benefit Analysis: Net Benefit by Candidate')\n",
    "plt.xticks(range(len(cba_df)), cba_df['formula'], rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig('cost_benefit_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "experimental-workflow"
   },
   "outputs": [],
   "source": [
    "# Create experimental validation workflow\n",
    "print(\"\\n=== EXPERIMENTAL VALIDATION WORKFLOW ===\")\n",
    "\n",
    "# Create prioritized workflow for top 10 materials\n",
    "experimental_workflow = create_experimental_workflow(priority_results, top_n=10)\n",
    "\n",
    "print(\"Prioritized experimental validation workflow:\")\n",
    "print(\"=\" * 60)\n",
    "print(experimental_workflow.to_string(index=False))\n",
    "\n",
    "# Workflow summary\n",
    "print(f\"\\nWorkflow Summary:\")\n",
    "print(f\"- Total materials in workflow: {len(experimental_workflow)}\")\n",
    "print(f\"- Batches: {experimental_workflow['batch_number'].max()}\")\n",
    "print(f\"- High priority materials: {len(experimental_workflow[experimental_workflow['priority_level'] == 'High'])}\")\n",
    "print(f\"- Medium priority materials: {len(experimental_workflow[experimental_workflow['priority_level'] == 'Medium'])}\")\n",
    "print(f\"- Low priority materials: {len(experimental_workflow[experimental_workflow['priority_level'] == 'Low'])}\")\n",
    "\n",
    "# Time estimate\n",
    "total_time = experimental_workflow['estimated_time_days'].sum()\n",
    "print(f\"- Estimated total time: {total_time:.1f} days\")\n",
    "print(f\"- Average time per material: {experimental_workflow['estimated_time_days'].mean():.1f} days\")\n",
    "\n",
    "# Export workflow\n",
    "experimental_workflow.to_csv('experimental_workflow.csv', index=False)\n",
    "print(\"\\nWorkflow exported to 'experimental_workflow.csv'\")\n",
    "\n",
    "# Final recommendations\n",
    "print(\"\\n=== FINAL SYNTHESIZABILITY VALIDATION SUMMARY ===\")\n",
    "print(\"üß™ SYNTHESIZABILITY WORKSHOP COMPLETED! üß™\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Started with: {len(generated_df)} ML-generated materials\")\n",
    "print(f\"Thermodynamically stable: {len(stable_materials)} ({len(stable_materials)/len(generated_df)*100:.1f}%)\")\n",
    "print(f\"Predicted synthesizable: {synthesis_results['ensemble_prediction'].sum()} ({synthesis_results['ensemble_prediction'].mean()*100:.1f}%)\")\n",
    "print(f\"High-priority candidates: {len(top_candidates)} materials prioritized for synthesis\")\n",
    "print(f\"Positive ROI candidates: {len(profitable_candidates)} materials with net benefit > $0\")\n",
    "print(\"\\nüìã EXPERIMENTAL WORKFLOW READY\")\n",
    "print(\"Next steps:\")\n",
    "print(\"1. Begin synthesis with high-priority candidates\")\n",
    "print(\"2. Validate predictions experimentally\")\n",
    "print(\"3. Update ML models with new experimental data\")\n",
    "print(\"4. Scale up production-ready materials\")\n",
    "\n",
    "print(\"\\nüöÄ This completes the end-to-end materials discovery pipeline!\")\n",
    "print(\"From data to lab synthesis - accelerated by machine learning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary-section"
   },
   "source": [
    "## üéâ Workshop Summary\n",
    "\n",
    "Congratulations! You've successfully completed the Materials Discovery Workshop with real data integration and synthesizability validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "final-summary",
    "outputId": "9e9a92f5-f305-4bfb-853c-2e37c1ba2ebd"
   },
   "outputs": [],
   "source": [
    "# Workshop summary\n",
    "print(\"üéä MATERIALS DISCOVERY WORKSHOP COMPLETED! üéä\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"üìä Data Source: {data_type.upper()}\")\n",
    "print(f\"üìö Training Materials: {len(ml_features)}\")\n",
    "print(f\"üé® Generated Materials: {len(generated_df)}\")\n",
    "print(f\"üß† VAE Latent Dimension: {model.latent_dim}\")\n",
    "print(f\"üìà Training Epochs: {params['epochs']}\")\n",
    "\n",
    "print(\"\\n‚úÖ Key Achievements:\")\n",
    "if data_type == 'real':\n",
    "    print(\"  ‚Ä¢ Integrated real Materials Project data\")\n",
    "    print(\"  ‚Ä¢ Trained ML model on verified materials\")\n",
    "    print(\"  ‚Ä¢ Generated potentially synthesizable materials\")\n",
    "    print(\"  ‚Ä¢ Connected to production materials database\")\n",
    "else:\n",
    "    print(\"  ‚Ä¢ Mastered VAE for materials generation\")\n",
    "    print(\"  ‚Ä¢ Learned ML concepts with synthetic data\")\n",
    "    print(\"  ‚Ä¢ Explored materials property relationships\")\n",
    "    print(\"  ‚Ä¢ Set up foundation for real data integration\")\n",
    "\n",
    "print(\"\\nüöÄ Next Steps:\")\n",
    "print(\"  ‚Ä¢ Experiment with different VAE architectures\")\n",
    "print(\"  ‚Ä¢ Try Materials Project data for production use\")\n",
    "print(\"  ‚Ä¢ Validate generated materials experimentally\")\n",
    "print(\"  ‚Ä¢ Explore reinforcement learning for property optimization\")\n",
    "\n",
    "print(\"\\nüî¨ Science Impact:\")\n",
    "print(\"  ‚Ä¢ Accelerated materials discovery workflow\")\n",
    "print(\"  ‚Ä¢ AI-assisted alloy design\")\n",
    "print(\"  ‚Ä¢ Integration of ML with materials databases\")\n",
    "print(\"  ‚Ä¢ Foundation for autonomous materials R&D\")\n",
    "\n",
    "print(\"\\nüí° Remember: The future of materials science is AI-augmented! üöÄ\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "6417d7434dd44b0c9a9410d71d2d3078": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "SliderStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "SliderStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": "",
       "handle_color": null
      }
     },
     "64f631411f914846b5bee50534fc99fa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6648ed737ab349fdac1455faa02df382": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "76ebce82bb0446c4b0552250bf2806b5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "SliderStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "SliderStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": "",
       "handle_color": null
      }
     },
     "82e7dee5172b4d29b282271f79e47621": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": "initial"
      }
     },
     "945d3ada9c7843a2935c2a91cf3494f5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "95a28574cbdd49d38d6a4913a76c570c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DropdownModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DropdownModel",
       "_options_labels": [
        "Synthetic (Demo)",
        "Materials Project (Real)"
       ],
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "DropdownView",
       "description": "Data Source:",
       "description_tooltip": null,
       "disabled": false,
       "index": 1,
       "layout": "IPY_MODEL_bf83cc9f9e8c4fdb8a12c3a2080c3a83",
       "style": "IPY_MODEL_82e7dee5172b4d29b282271f79e47621"
      }
     },
     "9c3a87c534dd4a418eeabbc8a44e79ef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntSliderModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntSliderModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "IntSliderView",
       "continuous_update": true,
       "description": "Latent Dim:",
       "description_tooltip": null,
       "disabled": false,
       "layout": "IPY_MODEL_64f631411f914846b5bee50534fc99fa",
       "max": 20,
       "min": 2,
       "orientation": "horizontal",
       "readout": true,
       "readout_format": "d",
       "step": 1,
       "style": "IPY_MODEL_deee3fc3be734f59858a73d7618956f2",
       "value": 5
      }
     },
     "bb0dd304b5a1426fabbc0989ab819193": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntSliderModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntSliderModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "IntSliderView",
       "continuous_update": true,
       "description": "Samples:",
       "description_tooltip": null,
       "disabled": false,
       "layout": "IPY_MODEL_945d3ada9c7843a2935c2a91cf3494f5",
       "max": 500,
       "min": 10,
       "orientation": "horizontal",
       "readout": true,
       "readout_format": "d",
       "step": 10,
       "style": "IPY_MODEL_76ebce82bb0446c4b0552250bf2806b5",
       "value": 100
      }
     },
     "bf83cc9f9e8c4fdb8a12c3a2080c3a83": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "deee3fc3be734f59858a73d7618956f2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "SliderStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "SliderStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": "",
       "handle_color": null
      }
     },
     "e65b68b7cba8424eab5e785096e0f8e6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntSliderModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntSliderModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "IntSliderView",
       "continuous_update": true,
       "description": "Epochs:",
       "description_tooltip": null,
       "disabled": false,
       "layout": "IPY_MODEL_6648ed737ab349fdac1455faa02df382",
       "max": 200,
       "min": 10,
       "orientation": "horizontal",
       "readout": true,
       "readout_format": "d",
       "step": 10,
       "style": "IPY_MODEL_6417d7434dd44b0c9a9410d71d2d3078",
       "value": 50
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
