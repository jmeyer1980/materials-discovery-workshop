{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "materials-discovery-workshop"
   },
   "source": [
    "# Materials Discovery Workshop - Google Colab Edition with Materials Project Integration\n",
    "\n",
    "This interactive notebook demonstrates how machine learning can accelerate materials discovery by learning patterns from existing alloy compositions and generating new ones.\n",
    "\n",
    "**New Feature**: Integration with the Materials Project database for real materials data!\n",
    "\n",
    "**Workshop Goals:**\n",
    "- Understand how variational autoencoders (VAEs) can model materials data\n",
    "- Learn to generate new alloy compositions using ML\n",
    "- Explore materials clustering and property analysis\n",
    "- See how AI can accelerate materials R&D\n",
    "- **NEW**: Use real materials data from Materials Project\n",
    "\n",
    "**What you'll need:**\n",
    "- Basic understanding of alloys and material properties\n",
    "- Curiosity about how ML can help with materials science\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-and-data-loading"
   },
   "source": [
    "## Step 1: Setup and Data Loading\n",
    "\n",
    "First, let's install dependencies and load our materials dataset. This workshop now supports both synthetic data (for demos) and real Materials Project data (for production use)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-dependencies"
   },
   "outputs": [],
   "source": [
    "# Install required packages for Colab\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install scikit-learn matplotlib seaborn pandas numpy ipywidgets pymatgen requests\n",
    "\n",
    "print(\"‚úÖ Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import-libraries"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from scipy.stats import ks_2samp\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Running on: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data-source-selection"
   },
   "source": [
    "## üÜï NEW: Choose Your Data Source\n",
    "\n",
    "This workshop now supports two data sources:\n",
    "\n",
    "1. **Synthetic Data** (Original): Programmatically generated for demonstrations\n",
    "2. **Materials Project Data** (NEW): Real materials from computational database\n",
    "\n",
    "Choose your data source below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data-source-choice"
   },
   "outputs": [],
   "source": [
    "# Data source selection\n",
    "data_source = widgets.Dropdown(\n",
    "    options=['Synthetic (Demo)', 'Materials Project (Real)'],\n",
    "    value='Materials Project (Real)',\n",
    "    description='Data Source:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "display(data_source)\n",
    "\n",
    "print(\"\\nüéØ Selected data source will be loaded in the next cell.\")\n",
    "print(\"   - Synthetic: Fast, good for learning concepts\")\n",
    "print(\"   - Materials Project: Real data, production-ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-materials-data"
   },
   "outputs": [],
   "source": [
    "# Materials Project API Integration Class\n",
    "class MaterialsProjectClient:\n",
    "    \"\"\"Client for Materials Project API with rate limiting and error handling.\"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str = \"pkHkQjeWQe8lFY29NV2p1yQ52rBKX3KE\"):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"https://api.materialsproject.org\"\n",
    "        self.last_request_time = 0\n",
    "        self.rate_limit_delay = 0.2\n",
    "        self.max_retries = 3\n",
    "\n",
    "    def _rate_limit_wait(self):\n",
    "        current_time = time.time()\n",
    "        time_since_last = current_time - self.last_request_time\n",
    "        if time_since_last < self.rate_limit_delay:\n",
    "            time.sleep(self.rate_limit_delay - time_since_last)\n",
    "        self.last_request_time = time.time()\n",
    "\n",
    "    def _make_request(self, endpoint: str, params: Dict = None) -> Dict:\n",
    "        if params is None:\n",
    "            params = {}\n",
    "        headers = {\"X-API-Key\": self.api_key}\n",
    "\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                self._rate_limit_wait()\n",
    "                response = requests.get(f\"{self.base_url}{endpoint}\", params=params, headers=headers, timeout=30)\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    return response.json()\n",
    "                elif response.status_code == 429:\n",
    "                    time.sleep(5)\n",
    "                    continue\n",
    "                else:\n",
    "                    if attempt < self.max_retries - 1:\n",
    "                        time.sleep(2 ** attempt)\n",
    "                        continue\n",
    "                    raise Exception(f\"API error {response.status_code}\")\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                if attempt < self.max_retries - 1:\n",
    "                    time.sleep(2 ** attempt)\n",
    "                    continue\n",
    "                raise\n",
    "\n",
    "        raise Exception(\"All API attempts failed\")\n",
    "\n",
    "    def get_materials_summary(self, elements: List[str] = None, limit: int = 100) -> pd.DataFrame:\n",
    "        params = {\n",
    "            \"_fields\": \"material_id,formula_pretty,elements,nsites,volume,density,density_atomic,band_gap,energy_above_hull,formation_energy_per_atom,total_magnetization\",\n",
    "            \"_limit\": limit\n",
    "        }\n",
    "\n",
    "        if elements:\n",
    "            params[\"elements\"] = \",\".join(elements)\n",
    "\n",
    "        response = self._make_request(\"/materials/summary/\", params)\n",
    "        materials = response.get(\"data\", [])\n",
    "\n",
    "        if not materials:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        df = pd.DataFrame(materials)\n",
    "        df.rename(columns={'formula_pretty': 'formula'}, inplace=True)\n",
    "\n",
    "        numeric_cols = ['nsites', 'volume', 'density', 'atomic_density', 'band_gap', 'energy_above_hull', 'formation_energy_per_atom', 'total_magnetization']\n",
    "        for col in numeric_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "        return df\n",
    "\n",
    "    def get_binary_alloys(self, element_pairs: List[Tuple[str, str]] = None, limit_per_pair: int = 50) -> pd.DataFrame:\n",
    "        if element_pairs is None:\n",
    "            element_pairs = [('Al', 'Ti'), ('Al', 'V'), ('Al', 'Cr'), ('Al', 'Fe'), ('Al', 'Ni'), ('Al', 'Cu'),\n",
    "                            ('Ti', 'V'), ('Ti', 'Cr'), ('Ti', 'Fe'), ('Ti', 'Ni'), ('V', 'Cr'), ('Fe', 'Co'), ('Fe', 'Ni'), ('Co', 'Ni'), ('Ni', 'Cu')]\n",
    "\n",
    "        all_materials = []\n",
    "        for elem1, elem2 in element_pairs:\n",
    "            materials = self.get_materials_summary(elements=[elem1, elem2], limit=limit_per_pair)\n",
    "            if not materials.empty:\n",
    "                materials['element_1'] = elem1\n",
    "                materials['element_2'] = elem2\n",
    "                materials['alloy_type'] = 'binary'\n",
    "                all_materials.append(materials)\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        if not all_materials:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        combined_df = pd.concat(all_materials, ignore_index=True)\n",
    "        combined_df.drop_duplicates(subset='material_id', inplace=True)\n",
    "        return combined_df\n",
    "\n",
    "# Load selected data source\n",
    "if data_source.value == 'Materials Project (Real)':\n",
    "    print(\"üîÑ Loading REAL materials data from Materials Project...\")\n",
    "    \n",
    "    try:\n",
    "        client = MaterialsProjectClient()\n",
    "        \n",
    "        # Test connection\n",
    "        test_data = client.get_materials_summary(elements=[\"Al\", \"Ti\"], limit=5)\n",
    "        if test_data.empty:\n",
    "            raise Exception(\"API connection failed\")\n",
    "        \n",
    "        # Get full dataset\n",
    "        raw_data = client.get_binary_alloys(limit_per_pair=30)\n",
    "        \n",
    "        if raw_data.empty:\n",
    "            raise Exception(\"No materials retrieved\")\n",
    "        \n",
    "        # Convert to ML features\n",
    "        import pymatgen.core as mg\n",
    "        \n",
    "        features_df = raw_data.copy()\n",
    "        for idx, row in features_df.iterrows():\n",
    "            if 'elements' in row and row['elements']:\n",
    "                elements = row['elements']\n",
    "                electronegativities = []\n",
    "                atomic_radii = []\n",
    "                \n",
    "                for elem_symbol in elements:\n",
    "                    try:\n",
    "                        elem = mg.Element(elem_symbol)\n",
    "                        if hasattr(elem, 'X') and elem.X is not None:\n",
    "                            electronegativities.append(elem.X)\n",
    "                        if hasattr(elem, 'atomic_radius') and elem.atomic_radius is not None:\n",
    "                            atomic_radii.append(elem.atomic_radius)\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                features_df.loc[idx, 'electronegativity'] = np.mean(electronegativities) if electronegativities else 0\n",
    "                features_df.loc[idx, 'atomic_radius'] = np.mean(atomic_radii) if atomic_radii else 0\n",
    "        \n",
    "        features_df['composition_1'] = 0.5\n",
    "        features_df['composition_2'] = 0.5\n",
    "        features_df['composition_3'] = 0.0\n",
    "        \n",
    "        ml_features = features_df[['composition_1', 'composition_2', 'composition_3', 'density', 'electronegativity', 'atomic_radius', 'band_gap', 'energy_above_hull', 'formation_energy_per_atom']].copy()\n",
    "        ml_features.rename(columns={'formation_energy_per_atom': 'melting_point'}, inplace=True)\n",
    "        ml_features.fillna(ml_features.mean(), inplace=True)\n",
    "        \n",
    "        ml_features['melting_point'] = ml_features['melting_point'].clip(-10, 10)\n",
    "        ml_features['density'] = ml_features['density'].clip(0, 50)\n",
    "        \n",
    "        data = features_df\n",
    "        data_type = \"real\"\n",
    "        \n",
    "        print(f\"‚úÖ Loaded {len(ml_features)} REAL materials from Materials Project!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load Materials Project data: {e}\")\n",
    "        print(\"Falling back to synthetic data...\")\n",
    "        data_source.value = 'Synthetic (Demo)'\n",
    "\n",
    "if data_source.value == 'Synthetic (Demo)':\n",
    "    print(\"üîÑ Creating SYNTHETIC materials dataset for demonstration...\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    \n",
    "    alloys = []\n",
    "    elements = ['Al', 'Ti', 'V', 'Cr', 'Mn', 'Fe', 'Co', 'Ni', 'Cu', 'Zn']\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        alloy_type = np.random.choice(['binary', 'ternary'], p=[0.7, 0.3])\n",
    "        \n",
    "        if alloy_type == 'binary':\n",
    "            elem1, elem2 = np.random.choice(elements, 2, replace=False)\n",
    "            comp1 = np.random.uniform(0.1, 0.9)\n",
    "            comp2 = 1 - comp1\n",
    "            comp3 = 0\n",
    "        else:\n",
    "            elem1, elem2, elem3 = np.random.choice(elements, 3, replace=False)\n",
    "            comp1 = np.random.uniform(0.1, 0.6)\n",
    "            comp2 = np.random.uniform(0.1, 0.6)\n",
    "            comp3 = 1 - comp1 - comp2\n",
    "        \n",
    "        melting_point = np.random.normal(1500, 300)\n",
    "        density = np.random.normal(7.8, 2.0)\n",
    "        electronegativity = np.random.normal(1.8, 0.3)\n",
    "        atomic_radius = np.random.normal(1.3, 0.2)\n",
    "        \n",
    "        alloys.append({\n",
    "            'id': f'alloy_{i+1}',\n",
    "            'alloy_type': alloy_type,\n",
    "            'element_1': elem1,\n",
    "            'element_2': elem2,\n",
    "            'element_3': elem3 if alloy_type == 'ternary' else None,\n",
    "            'composition_1': comp1,\n",
    "            'composition_2': comp2,\n",
    "            'composition_3': comp3,\n",
    "            'melting_point': max(500, melting_point),\n",
    "            'density': max(2, density),\n",
    "            'electronegativity': max(0.7, min(2.5, electronegativity)),\n",
    "            'atomic_radius': max(1.0, min(1.8, atomic_radius))\n",
    "        })\n",
    "    \n",
    "    data = pd.DataFrame(alloys)\n",
    "    data_type = \"synthetic\"\n",
    "    \n",
    "    # Create ML features from synthetic data\n",
    "    binary_data_synth = data[data['alloy_type'] == 'binary'].copy()\n",
    "    binary_data_synth['composition_3'] = binary_data_synth['composition_3'].fillna(0)\n",
    "    ml_features = binary_data_synth[['composition_1', 'composition_2', 'composition_3', 'melting_point', 'density', 'electronegativity', 'atomic_radius']].copy()\n",
    "    ml_features['band_gap'] = 0.0\n",
    "    ml_features['energy_above_hull'] = 0.0\n",
    "    \n",
    "    print(f\"‚úÖ Created {len(ml_features)} SYNTHETIC materials for demonstration!\")\n",
    "\n",
    "print(f\"\\nüìä Dataset ready: {len(ml_features)} materials ({data_type} data)\")\n",
    "print(\"First few rows:\")\n",
    "display_cols = ['alloy_type', 'element_1', 'element_2', 'density', 'melting_point'] if data_type == 'synthetic' else ['formula', 'elements', 'density', 'band_gap']\n",
    "print(data[display_cols].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "explore-dataset"
   },
   "outputs": [],
   "source": [
    "# Explore the loaded dataset\n",
    "print(\"üìä DATASET EXPLORATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if data_type == 'real':\n",
    "    print(\"Alloy types distribution:\")\n",
    "    print(data['alloy_type'].value_counts())\n",
    "    print(\"\\nProperty statistics:\")\n",
    "    print(data[['density', 'band_gap', 'energy_above_hull']].describe())\n",
    "    \n",
    "    # Show unique element combinations\n",
    "    element_pairs = data.apply(lambda x: f\"{x['element_1']}-{x['element_2']}\", axis=1)\n",
    "    print(\"\\nTop element combinations:\")\n",
    "    print(element_pairs.value_counts().head(10))\n",
    "    \n",
    "else:\n",
    "    print(\"Alloy types distribution:\")\n",
    "    print(data['alloy_type'].value_counts())\n",
    "    print(\"\\nProperty statistics:\")\n",
    "    print(data[['melting_point', 'density', 'electronegativity', 'atomic_radius']].describe())\n",
    "\n",
    "# Data quality check\n",
    "missing_values = ml_features.isnull().sum().sum()\n",
    "print(f\"\\nMissing values in dataset: {missing_values}\")\n",
    "print(f\"Data shape: {ml_features.shape}\")\n",
    "print(f\"Features: {list(ml_features.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "interactive-parameters"
   },
   "source": [
    "## Interactive Parameters\n",
    "\n",
    "Let's set up some interactive controls to experiment with different model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "parameter-controls"
   },
   "outputs": [],
   "source": [
    "# Interactive parameter controls\n",
    "latent_dim_slider = widgets.IntSlider(value=5, min=2, max=20, step=1, description='Latent Dim:')\n",
    "epochs_slider = widgets.IntSlider(value=50, min=10, max=200, step=10, description='Epochs:')\n",
    "num_samples_slider = widgets.IntSlider(value=100, min=10, max=500, step=10, description='Samples:')\n",
    "\n",
    "display(latent_dim_slider, epochs_slider, num_samples_slider)\n",
    "\n",
    "# Global parameters (will be updated by widgets)\n",
    "params = {\n",
    "    'latent_dim': latent_dim_slider.value,\n",
    "    'epochs': epochs_slider.value,\n",
    "    'num_samples': num_samples_slider.value\n",
    "}\n",
    "\n",
    "def update_params(change):\n",
    "    params['latent_dim'] = latent_dim_slider.value\n",
    "    params['epochs'] = epochs_slider.value\n",
    "    params['num_samples'] = num_samples_slider.value\n",
    "    print(f\"Updated parameters: {params}\")\n",
    "\n",
    "latent_dim_slider.observe(update_params, names='value')\n",
    "epochs_slider.observe(update_params, names='value')\n",
    "num_samples_slider.observe(update_params, names='value')\n",
    "\n",
    "print(\"Interactive controls ready! Adjust the sliders and rerun cells below.\")\n",
    "print(f\"\\nüéØ Training on {data_type.upper()} data with {len(ml_features)} materials!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data-preprocessing"
   },
   "source": [
    "## Step 2: Data Preprocessing\n",
    "\n",
    "We need to prepare our data for machine learning. This involves:\n",
    "- Selecting relevant features\n",
    "- Handling missing values\n",
    "- Scaling the data\n",
    "\n",
    "Let's focus on binary alloys for this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "preprocess-data"
   },
   "outputs": [],
   "source": [
    "# Select features and prepare for ML\n",
    "feature_cols = ['composition_1', 'composition_2', 'melting_point', 'density', 'electronegativity', 'atomic_radius']\n",
    "features = ml_features[feature_cols].values\n",
    "\n",
    "print(f\"Using {len(ml_features)} materials\")\n",
    "print(f\"Feature matrix shape: {features.shape}\")\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "print(\"Features scaled successfully!\")\n",
    "\n",
    "# Show feature statistics\n",
    "print(\"\\nFeature scaling statistics:\")\n",
    "scaled_df = pd.DataFrame(features_scaled, columns=feature_cols)\n",
    "print(scaled_df.describe().loc[['mean', 'std']].round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vae-architecture"
   },
   "source": [
    "## Step 3: The Variational Autoencoder (VAE)\n",
    "\n",
    "A VAE is a type of neural network that can learn to generate new data similar to its training data. Here's how it works:\n",
    "\n",
    "- **Encoder**: Compresses input data into a lower-dimensional latent space\n",
    "- **Latent Space**: A compressed representation where similar materials are close together\n",
    "- **Decoder**: Reconstructs data from the latent space\n",
    "\n",
    "The \"variational\" part means it learns a probability distribution, allowing us to sample new materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "define-vae"
   },
   "outputs": [],
   "source": [
    "class OptimizedVAE(nn.Module):\n",
    "    \"\"\"Optimized Variational Autoencoder for materials discovery with improved convergence.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int = 6, latent_dim: int = 5):\n",
    "        super(OptimizedVAE, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder - increased capacity for better convergence\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(32, latent_dim)\n",
    "        self.fc_var = nn.Linear(32, latent_dim)\n",
    "\n",
    "        # Decoder - symmetric to encoder, no sigmoid for unbounded features\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_dim)\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        log_var = self.fc_var(h)\n",
    "        return mu, log_var\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        reconstructed = self.decode(z)\n",
    "        return reconstructed, mu, log_var\n",
    "\n",
    "print(\"VAE class defined successfully!\")\n",
    "print(f\"\\nüéØ Ready to train on {data_type.upper()} data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training-section"
   },
   "source": [
    "## Step 4: Training the VAE\n",
    "\n",
    "Now let's train our VAE on the selected dataset. The model will learn to compress and reconstruct materials data, enabling generation of new materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-vae"
   },
   "outputs": [],
   "source": [
    "# Initialize and train the optimized VAE\n",
    "try:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    input_dim = features_scaled.shape[1]\n",
    "    model = OptimizedVAE(input_dim=input_dim, latent_dim=params['latent_dim']).to(device)\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    features_tensor = torch.FloatTensor(features_scaled)\n",
    "    dataset = torch.utils.data.TensorDataset(features_tensor, features_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # Training setup\n",
    "    initial_lr = 0.005\n",
    "    optimizer = optim.Adam(model.parameters(), lr=initial_lr)\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.995)\n",
    "    epochs = params['epochs']\n",
    "\n",
    "    print(f\"üöÄ Training optimized VAE on {data_type.upper()} data for {epochs} epochs...\")\n",
    "    print(f\"üìä Dataset: {len(ml_features)} materials, {input_dim} features\")\n",
    "    print(f\"üß† Model: {input_dim} ‚Üí 64 ‚Üí 32 ‚Üí {model.latent_dim} ‚Üí 32 ‚Üí 64 ‚Üí {input_dim}\")\n",
    "    print(f\"‚ö° Running on: {device}\")\n",
    "    print(\"\\nThis may take a minute or two...\")\n",
    "\n",
    "    model.train()\n",
    "    losses = []\n",
    "    reconstruction_losses = []\n",
    "    kl_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        epoch_recon_loss = 0\n",
    "        epoch_kl_loss = 0\n",
    "        \n",
    "        for batch_x, _ in dataloader:\n",
    "            batch_x = batch_x.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            reconstructed, mu, log_var = model(batch_x)\n",
    "\n",
    "            # Compute losses\n",
    "            reconstruction_loss = nn.functional.mse_loss(reconstructed, batch_x, reduction='sum')\n",
    "            kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "            \n",
    "            kl_weight = min(1.0, epoch / 10.0)\n",
    "            loss = reconstruction_loss + kl_weight * kl_loss\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_recon_loss += reconstruction_loss.item()\n",
    "            epoch_kl_loss += kl_loss.item()\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        avg_recon_loss = epoch_recon_loss / len(dataloader)\n",
    "        avg_kl_loss = epoch_kl_loss / len(dataloader)\n",
    "        \n",
    "        losses.append(avg_loss)\n",
    "        reconstruction_losses.append(avg_recon_loss)\n",
    "        kl_losses.append(avg_kl_loss)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Total Loss: {avg_loss:.4f}, Recon: {avg_recon_loss:.4f}, KL: {avg_kl_loss:.4f}\")\n",
    "\n",
    "    print(f\"\\n‚úÖ VAE training completed on {data_type.upper()} data!\")\n",
    "    print(f\"üìà Final loss: {losses[-1]:.4f}\")\n",
    "    print(f\"üéØ Trained on {len(ml_features)} {data_type} materials\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training error: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "generation-section"
   },
   "source": [
    "## Step 5: Generating New Materials\n",
    "\n",
    "Now that we have a trained VAE, we can generate new materials by sampling from the latent space. This is like asking the model to \"imagine\" new alloys that follow the patterns it learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate-new-materials"
   },
   "outputs": [],
   "source": [
    "# Generate new materials\n",
    "model.eval()\n",
    "num_samples = params['num_samples']\n",
    "\n",
    "print(f\"üé® Generating {num_samples} new material compositions...\")\n",
    "print(f\"üìö Based on patterns learned from {data_type.upper()} data\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Sample from latent space\n",
    "    z = torch.randn(num_samples, model.latent_dim).to(device)\n",
    "    generated_features = model.decode(z).cpu().numpy()\n",
    "\n",
    "    # Inverse transform to original scale\n",
    "    generated_features = scaler.inverse_transform(generated_features)\n",
    "\n",
    "# Create DataFrame with generated materials\n",
    "elements = ['Al', 'Ti', 'V', 'Cr', 'Mn', 'Fe', 'Co', 'Ni', 'Cu', 'Zn']\n",
    "new_materials = []\n",
    "\n",
    "for i, features in enumerate(generated_features):\n",
    "    elem1, elem2 = random.sample(elements, 2)\n",
    "    comp1 = max(0.1, min(0.9, features[0]))\n",
    "    comp2 = 1.0 - comp1\n",
    "    \n",
    "    material = {\n",
    "        'id': f'generated_{i+1}',\n",
    "        'element_1': elem1,\n",
    "        'element_2': elem2,\n",
    "        'composition_1': comp1,\n",
    "        'composition_2': comp2,\n",
    "        'formula': f'{elem1}{comp1:.3f}{elem2}{comp2:.3f}',\n",
    "        'melting_point': abs(features[2]),\n",
    "        'density': abs(features[3]),\n",
    "        'electronegativity': max(0, features[4]),\n",
    "        'atomic_radius': max(0, features[5]),\n",
    "        'data_source': data_type,\n",
    "        'is_generated': True\n",
    "    }\n",
    "    new_materials.append(material)\n",
    "\n",
    "generated_df = pd.DataFrame(new_materials)\n",
    "print(f\"‚úÖ Generated {len(generated_df)} new materials!\")\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\nüß™ Example generated materials:\")\n",
    "display_cols = ['formula', 'melting_point', 'density']\n",
    "print(generated_df[display_cols].head(10))\n",
    "\n",
    "if data_type == 'real':\n",
    "    print(\"\\nüéØ These materials are generated based on REAL Materials Project data!\")\n",
    "    print(\"üî¨ They could potentially be synthesized and tested experimentally.\")\n",
    "else:\n",
    "    print(\"\\nüìö These materials are generated based on SYNTHETIC data patterns.\")\n",
    "    print(\"üß™ Great for learning ML concepts and testing workflows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary-section"
   },
   "source": [
    "## üéâ Workshop Summary\n",
    "\n",
    "Congratulations! You've successfully completed the Materials Discovery Workshop with real data integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "final-summary"
   },
   "outputs": [],
   "source": [
    "# Workshop summary\n",
    "print(\"üéä MATERIALS DISCOVERY WORKSHOP COMPLETED! üéä\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"üìä Data Source: {data_type.upper()}\")\n",
    "print(f\"üìö Training Materials: {len(ml_features)}\")\n",
    "print(f\"üé® Generated Materials: {len(generated_df)}\")\n",
    "print(f\"üß† VAE Latent Dimension: {model.latent_dim}\")\n",
    "print(f\"üìà Training Epochs: {params['epochs']}\")\n",
    "\n",
    "print(\"\\n‚úÖ Key Achievements:\")\n",
    "if data_type == 'real':\n",
    "    print(\"  ‚Ä¢ Integrated real Materials Project data\")\n",
    "    print(\"  ‚Ä¢ Trained ML model on verified materials\")\n",
    "    print(\"  ‚Ä¢ Generated potentially synthesizable materials\")\n",
    "    print(\"  ‚Ä¢ Connected to production materials database\")\n",
    "else:\n",
    "    print(\"  ‚Ä¢ Mastered VAE for materials generation\")\n",
    "    print(\"  ‚Ä¢ Learned ML concepts with synthetic data\")\n",
    "    print(\"  ‚Ä¢ Explored materials property relationships\")\n",
    "    print(\"  ‚Ä¢ Set up foundation for real data integration\")\n",
    "\n",
    "print(\"\\nüöÄ Next Steps:\")\n",
    "print(\"  ‚Ä¢ Experiment with different VAE architectures\")\n",
    "print(\"  ‚Ä¢ Try Materials Project data for production use\")\n",
    "print(\"  ‚Ä¢ Validate generated materials experimentally\")\n",
    "print(\"  ‚Ä¢ Explore reinforcement learning for property optimization\")\n",
    "\n",
    "print(\"\\nüî¨ Science Impact:\")\n",
    "print(\"  ‚Ä¢ Accelerated materials discovery workflow\")\n",
    "print(\"  ‚Ä¢ AI-assisted alloy design\")\n",
    "print(\"  ‚Ä¢ Integration of ML with materials databases\")\n",
    "print(\"  ‚Ä¢ Foundation for autonomous materials R&D\")\n",
    "\n",
    "print(\"\\nüí° Remember: The future of materials science is AI-augmented! üöÄ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
