{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Materials Discovery Workshop\n",
    "\n",
    "This interactive notebook demonstrates how machine learning can accelerate materials discovery by learning patterns from existing alloy compositions and generating new ones.\n",
    "\n",
    "**Workshop Goals:**\n",
    "- Understand how variational autoencoders (VAEs) can model materials data\n",
    "- Learn to generate new alloy compositions using ML\n",
    "- Explore materials clustering and property analysis\n",
    "- See how AI can accelerate materials R&D\n",
    "\n",
    "**What you'll need:**\n",
    "- Basic understanding of alloys and material properties\n",
    "- Curiosity about how ML can help with materials science\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Data Loading\n",
    "\n",
    "First, let's import the necessary libraries and load our materials dataset. This dataset contains alloy compositions and their properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "PyTorch version: 2.9.0+cpu\n",
      "Running on: CPU\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Running on: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Parameters\n",
    "\n",
    "Let's set up some interactive controls to experiment with different model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc2fd42740044ba5a5edc7d6d3d739f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=5, description='Latent Dim:', max=20, min=2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c52f54fc7a54a02951e6af2353c263f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=50, description='Epochs:', max=200, min=10, step=10)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "517393111ad44711b2bf0721603dadaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=100, description='Samples:', max=500, min=10, step=10)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive controls ready! Adjust the sliders and rerun cells below.\n"
     ]
    }
   ],
   "source": [
    "# Interactive parameter controls\n",
    "latent_dim_slider = widgets.IntSlider(value=5, min=2, max=20, step=1, description='Latent Dim:')\n",
    "epochs_slider = widgets.IntSlider(value=50, min=10, max=200, step=10, description='Epochs:')\n",
    "num_samples_slider = widgets.IntSlider(value=100, min=10, max=500, step=10, description='Samples:')\n",
    "\n",
    "display(latent_dim_slider, epochs_slider, num_samples_slider)\n",
    "\n",
    "# Global parameters (will be updated by widgets)\n",
    "params = {\n",
    "    'latent_dim': latent_dim_slider.value,\n",
    "    'epochs': epochs_slider.value,\n",
    "    'num_samples': num_samples_slider.value\n",
    "}\n",
    "\n",
    "def update_params(change):\n",
    "    params['latent_dim'] = latent_dim_slider.value\n",
    "    params['epochs'] = epochs_slider.value\n",
    "    params['num_samples'] = num_samples_slider.value\n",
    "    print(f\"Updated parameters: {params}\")\n",
    "\n",
    "latent_dim_slider.observe(update_params, names='value')\n",
    "epochs_slider.observe(update_params, names='value')\n",
    "num_samples_slider.observe(update_params, names='value')\n",
    "\n",
    "print(\"Interactive controls ready! Adjust the sliders and rerun cells below.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: materials_dataset.csv not found!\n",
      "Please ensure the dataset file is in the same directory as this notebook.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'materials_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1410060434.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load and explore the materials dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'materials_dataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataset shape: {data.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nFirst few rows:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'materials_dataset.csv'"
     ]
    }
   ],
   "source": [
    "# Load and explore the materials dataset\n",
    "try:\n",
    "    data = pd.read_csv('materials_dataset.csv')\n",
    "    print(f\"Dataset shape: {data.shape}\")\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    data.head()\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: materials_dataset.csv not found!\")\n",
    "    print(\"Please ensure the dataset file is in the same directory as this notebook.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading dataset: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore alloy types\n",
    "print(\"Alloy types distribution:\")\n",
    "print(data['alloy_type'].value_counts())\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nProperty statistics:\")\n",
    "data[['melting_point', 'density', 'electronegativity', 'atomic_radius']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Preprocessing\n",
    "\n",
    "We need to prepare our data for machine learning. This involves:\n",
    "- Selecting relevant features\n",
    "- Handling missing values\n",
    "- Scaling the data\n",
    "\n",
    "Let's focus on binary alloys for this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select binary alloys and key features\n",
    "binary_data = data[data['alloy_type'] == 'binary'].copy()\n",
    "binary_data['composition_3'] = binary_data['composition_3'].fillna(0)\n",
    "\n",
    "feature_cols = ['composition_1', 'composition_2', 'melting_point', \n",
    "               'density', 'electronegativity', 'atomic_radius']\n",
    "features = binary_data[feature_cols].values\n",
    "\n",
    "print(f\"Using {len(binary_data)} binary alloys\")\n",
    "print(f\"Feature matrix shape: {features.shape}\")\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "print(\"Features scaled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: The Variational Autoencoder (VAE)\n",
    "\n",
    "A VAE is a type of neural network that can learn to generate new data similar to its training data. Here's how it works:\n",
    "\n",
    "- **Encoder**: Compresses input data into a lower-dimensional latent space\n",
    "- **Latent Space**: A compressed representation where similar materials are close together\n",
    "- **Decoder**: Reconstructs data from the latent space\n",
    "\n",
    "The \"variational\" part means it learns a probability distribution, allowing us to sample new materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedVAE(nn.Module):\n",
    "    \"\"\"Optimized Variational Autoencoder for materials discovery with improved convergence.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int = 6, latent_dim: int = 5):\n",
    "        super(OptimizedVAE, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder - increased capacity for better convergence\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(32, latent_dim)\n",
    "        self.fc_var = nn.Linear(32, latent_dim)\n",
    "\n",
    "        # Decoder - symmetric to encoder, no sigmoid for unbounded features\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_dim)\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        log_var = self.fc_var(h)\n",
    "        return mu, log_var\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        reconstructed = self.decode(z)\n",
    "        return reconstructed, mu, log_var\n",
    "\n",
    "print(\"VAE class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the optimized VAE\n",
    "try:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    input_dim = features_scaled.shape[1]\n",
    "    model = OptimizedVAE(input_dim=input_dim, latent_dim=params['latent_dim']).to(device)\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    features_tensor = torch.FloatTensor(features_scaled)\n",
    "    dataset = torch.utils.data.TensorDataset(features_tensor, features_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # Training setup with improved hyperparameters\n",
    "    initial_lr = 0.005  # Higher initial learning rate for faster convergence\n",
    "    optimizer = optim.Adam(model.parameters(), lr=initial_lr)\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.995)  # Gradual decay\n",
    "    epochs = params['epochs']\n",
    "\n",
    "    print(f\"Training optimized VAE for {epochs} epochs on {device}...\")\n",
    "    print(f\"Model capacity: {model.input_dim} -> 64 -> 32 -> {model.latent_dim} -> 32 -> 64 -> {model.input_dim}\")\n",
    "    print(\"This may take a minute or two...\")\n",
    "\n",
    "    model.train()\n",
    "    losses = []\n",
    "    reconstruction_losses = []\n",
    "    kl_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        epoch_recon_loss = 0\n",
    "        epoch_kl_loss = 0\n",
    "        \n",
    "        for batch_x, _ in dataloader:\n",
    "            batch_x = batch_x.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            reconstructed, mu, log_var = model(batch_x)\n",
    "\n",
    "            # Compute losses with better weighting\n",
    "            reconstruction_loss = nn.functional.mse_loss(reconstructed, batch_x, reduction='sum')\n",
    "            kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "            \n",
    "            # KL annealing for better convergence\n",
    "            kl_weight = min(1.0, epoch / 10.0)  # Gradually increase KL weight\n",
    "            loss = reconstruction_loss + kl_weight * kl_loss\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_recon_loss += reconstruction_loss.item()\n",
    "            epoch_kl_loss += kl_loss.item()\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        avg_recon_loss = epoch_recon_loss / len(dataloader)\n",
    "        avg_kl_loss = epoch_kl_loss / len(dataloader)\n",
    "        \n",
    "        losses.append(avg_loss)\n",
    "        reconstruction_losses.append(avg_recon_loss)\n",
    "        kl_losses.append(avg_kl_loss)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Total Loss: {avg_loss:.4f}, Recon: {avg_recon_loss:.4f}, KL: {avg_kl_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "    print(\"\\nOptimized VAE training completed!\")\n",
    "    print(f\"Final loss: {losses[-1]:.4f} (improvement: {losses[0] - losses[-1]:.4f})\")\n",
    "\n",
    "    # Plot detailed training metrics\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    axes[0].plot(losses, label='Total Loss')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Total Training Loss')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].legend()\n",
    "    \n",
    "    axes[1].plot(reconstruction_losses, label='Reconstruction Loss', color='orange')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].set_title('Reconstruction Loss')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].legend()\n",
    "    \n",
    "    axes[2].plot(kl_losses, label='KL Divergence', color='green')\n",
    "    axes[2].set_xlabel('Epoch')\n",
    "    axes[2].set_ylabel('Loss')\n",
    "    axes[2].set_title('KL Divergence Loss')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    axes[2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "except RuntimeError as e:\n",
    "    if 'CUDA' in str(e):\n",
    "        print(\"ERROR: CUDA/GPU error occurred!\")\n",
    "        print(\"Try switching to CPU or reducing model size.\")\n",
    "        print(f\"Details: {e}\")\n",
    "    else:\n",
    "        print(f\"ERROR during training: {e}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during VAE setup/training: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generating New Materials\n",
    "\n",
    "Now that we have a trained VAE, we can generate new materials by sampling from the latent space. This is like asking the model to \"imagine\" new alloys that follow the patterns it learned from existing materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new materials\n",
    "model.eval()\n",
    "num_samples = params['num_samples']  # Controlled by slider\n",
    "\n",
    "print(f\"Generating {num_samples} new material compositions...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Sample from latent space\n",
    "    z = torch.randn(num_samples, model.latent_dim).to(device)\n",
    "    generated_features = model.decode(z).cpu().numpy()\n",
    "\n",
    "    # Inverse transform to original scale\n",
    "    generated_features = scaler.inverse_transform(generated_features)\n",
    "\n",
    "# Create DataFrame with generated materials\n",
    "elements = ['Al', 'Ti', 'V', 'Cr', 'Mn', 'Fe', 'Co', 'Ni', 'Cu', 'Zn']\n",
    "new_materials = []\n",
    "\n",
    "for i, features in enumerate(generated_features):\n",
    "    elem1, elem2 = random.sample(elements, 2)\n",
    "    comp1 = max(0.1, min(0.9, features[0]))\n",
    "    comp2 = 1.0 - comp1\n",
    "    \n",
    "    material = {\n",
    "        'id': f'generated_{i+1}',\n",
    "        'element_1': elem1,\n",
    "        'element_2': elem2,\n",
    "        'composition_1': comp1,\n",
    "        'composition_2': comp2,\n",
    "        'formula': f'{elem1}{comp1:.3f}{elem2}{comp2:.3f}',\n",
    "        'melting_point': abs(features[2]),\n",
    "        'density': abs(features[3]),\n",
    "        'electronegativity': max(0, features[4]),\n",
    "        'atomic_radius': max(0, features[5]),\n",
    "        'is_generated': True\n",
    "    }\n",
    "    new_materials.append(material)\n",
    "\n",
    "generated_df = pd.DataFrame(new_materials)\n",
    "print(f\"Generated {len(generated_df)} new materials!\")\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\nExample generated materials:\")\n",
    "generated_df[['formula', 'melting_point', 'density']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Model Validation and Generation Analysis\n",
    "\n",
    "Let's evaluate the quality of our trained VAE model and analyze the diversity of the generated materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# Evaluate model performance and generation quality\n",
    "model.eval()\n",
    "\n",
    "# 1. Reconstruction quality on training data\n",
    "print(\"=== MODEL VALIDATION ===\")\n",
    "with torch.no_grad():\n",
    "    train_reconstructed, _, _ = model(features_tensor.to(device))\n",
    "    train_reconstruction_error = nn.functional.mse_loss(train_reconstructed, features_tensor.to(device))\n",
    "    print(f\"Training reconstruction MSE: {train_reconstruction_error.item():.6f}\")\n",
    "\n",
    "# 2. Generation diversity analysis\n",
    "print(\"\\n=== GENERATION DIVERSITY ANALYSIS ===\")\n",
    "\n",
    "# Generate larger sample for analysis\n",
    "large_sample_size = min(1000, len(binary_data))\n",
    "with torch.no_grad():\n",
    "    z_large = torch.randn(large_sample_size, model.latent_dim).to(device)\n",
    "    generated_large = model.decode(z_large).cpu().numpy()\n",
    "    generated_large = scaler.inverse_transform(generated_large)\n",
    "\n",
    "# Calculate statistics\n",
    "original_stats = binary_data[['melting_point', 'density', 'electronegativity', 'atomic_radius']].describe()\n",
    "generated_stats = pd.DataFrame(generated_large[:, 2:], \n",
    "                              columns=['melting_point', 'density', 'electronegativity', 'atomic_radius']).describe()\n",
    "\n",
    "print(\"Original Materials Statistics:\")\n",
    "print(original_stats.loc[['mean', 'std']].round(3))\n",
    "print(\"\\nGenerated Materials Statistics:\")\n",
    "print(generated_stats.loc[['mean', 'std']].round(3))\n",
    "\n",
    "# Coverage analysis - how well generated materials cover the original distribution\n",
    "\n",
    "properties = ['melting_point', 'density', 'electronegativity', 'atomic_radius']\n",
    "coverage_scores = {}\n",
    "\n",
    "for prop in properties:\n",
    "    original_vals = binary_data[prop].values\n",
    "    generated_vals = generated_large[:, feature_cols.index(prop)]\n",
    "    \n",
    "    # Kolmogorov-Smirnov test for distribution similarity\n",
    "    ks_stat, p_value = ks_2samp(original_vals, generated_vals)\n",
    "    coverage_scores[prop] = {'ks_stat': ks_stat, 'p_value': p_value}\n",
    "    \n",
    "print(\"\\nDistribution Similarity (KS Test):\")\n",
    "for prop, scores in coverage_scores.items():\n",
    "    print(f\"{prop}: KS-stat={scores['ks_stat']:.3f}, p-value={scores['p_value']:.3f}\")\n",
    "\n",
    "# Novelty check - how many generated materials are outside training range\n",
    "novelty_count = 0\n",
    "for i, gen_features in enumerate(generated_large):\n",
    "    is_novel = False\n",
    "    for j, prop in enumerate(properties):\n",
    "        prop_idx = feature_cols.index(prop)\n",
    "        gen_val = gen_features[prop_idx]\n",
    "        orig_min, orig_max = binary_data[prop].min(), binary_data[prop].max()\n",
    "        if gen_val < orig_min * 0.9 or gen_val > orig_max * 1.1:  # 10% margin\n",
    "            is_novel = True\n",
    "        novelty_count += 1\n",
    "\n",
    "print(f\"\\nNovelty Analysis: {novelty_count}/{large_sample_size} ({novelty_count/large_sample_size*100:.1f}%) generated materials extend beyond training range\")\n",
    "\n",
    "# Latent space analysis\n",
    "print(\"\\n=== LATENT SPACE ANALYSIS ===\")\n",
    "with torch.no_grad():\n",
    "    _, mu, log_var = model(features_tensor.to(device))\n",
    "    mu_np = mu.cpu().numpy()\n",
    "    log_var_np = log_var.cpu().numpy()\n",
    "\n",
    "print(f\"Latent space dimension: {model.latent_dim}\")\n",
    "print(f\"Latent means - Mean: {mu_np.mean():.3f}, Std: {mu_np.std():.3f}\")\n",
    "print(f\"Latent variances - Mean: {log_var_np.mean():.3f}, Std: {log_var_np.std():.3f}\")\n",
    "\n",
    "# Plot latent space if 2D\n",
    "if model.latent_dim == 2:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(mu_np[:, 0], mu_np[:, 1], alpha=0.6, c='blue', s=30)\n",
    "    plt.xlabel('Latent Dimension 1')\n",
    "    plt.ylabel('Latent Dimension 2')\n",
    "    plt.title('Training Data in Latent Space')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nModel validation completed!\")\n",
    "\n",
    "print(\"\\nModel validation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Materials Clustering\n",
    "\n",
    "Let's analyze the original materials by grouping them into clusters. This helps us understand natural groupings in the materials space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform clustering analysis\n",
    "pca = PCA(n_components=2)\n",
    "features_pca = pca.fit_transform(features_scaled)\n",
    "\n",
    "# Find optimal number of clusters\n",
    "silhouette_scores = []\n",
    "for n_clusters in range(2, 8):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(features_pca)\n",
    "    silhouette_avg = silhouette_score(features_pca, cluster_labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "optimal_clusters = silhouette_scores.index(max(silhouette_scores)) + 2\n",
    "print(f\"Optimal number of clusters: {optimal_clusters}\")\n",
    "\n",
    "# Perform clustering\n",
    "kmeans = KMeans(n_clusters=optimal_clusters, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(features_pca)\n",
    "\n",
    "# Add cluster info to data\n",
    "clustered_data = binary_data.copy()\n",
    "clustered_data['cluster'] = cluster_labels\n",
    "clustered_data['pca_1'] = features_pca[:, 0]\n",
    "clustered_data['pca_2'] = features_pca[:, 1]\n",
    "\n",
    "print(f\"Materials grouped into {optimal_clusters} clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Visualizations and Analysis\n",
    "\n",
    "Let's create visualizations to compare original and generated materials, and explore the clustering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Materials Discovery ML Workshop Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Melting Point vs Density\n",
    "ax = axes[0, 0]\n",
    "ax.scatter(binary_data['density'], binary_data['melting_point'], \n",
    "           alpha=0.6, c='blue', label='Original Materials', s=30)\n",
    "ax.scatter(generated_df['density'], generated_df['melting_point'], \n",
    "           alpha=0.8, c='red', marker='x', label='Generated Materials', s=50)\n",
    "ax.set_xlabel('Density (g/cm³)')\n",
    "ax.set_ylabel('Melting Point (K)')\n",
    "ax.set_title('Material Properties: Original vs Generated')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Composition Space\n",
    "ax = axes[0, 1]\n",
    "ax.scatter(binary_data['composition_1'], binary_data['composition_2'], \n",
    "           alpha=0.6, c='blue', label='Original', s=30)\n",
    "ax.scatter(generated_df['composition_1'], generated_df['composition_2'], \n",
    "           alpha=0.8, c='red', marker='x', label='Generated', s=50)\n",
    "ax.set_xlabel('Element 1 Composition')\n",
    "ax.set_ylabel('Element 2 Composition')\n",
    "ax.set_title('Composition Space')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Cluster Analysis\n",
    "ax = axes[0, 2]\n",
    "scatter = ax.scatter(clustered_data['pca_1'], clustered_data['pca_2'], \n",
    "                     c=clustered_data['cluster'], cmap='viridis', alpha=0.7, s=40)\n",
    "ax.set_xlabel('PCA Component 1')\n",
    "ax.set_ylabel('PCA Component 2')\n",
    "ax.set_title('Material Clusters (PCA)')\n",
    "plt.colorbar(scatter, ax=ax, label='Cluster')\n",
    "\n",
    "# 4. Property Distributions - Melting Point\n",
    "ax = axes[1, 0]\n",
    "ax.hist(binary_data['melting_point'], bins=20, alpha=0.7, color='blue', \n",
    "        label='Original', density=True)\n",
    "ax.hist(generated_df['melting_point'], bins=20, alpha=0.7, color='red', \n",
    "        label='Generated', density=True)\n",
    "ax.set_xlabel('Melting Point (K)')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Melting Point Distribution')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Property Distributions - Density\n",
    "ax = axes[1, 1]\n",
    "ax.hist(binary_data['density'], bins=20, alpha=0.7, color='blue', \n",
    "        label='Original', density=True)\n",
    "ax.hist(generated_df['density'], bins=20, alpha=0.7, color='red', \n",
    "        label='Generated', density=True)\n",
    "ax.set_xlabel('Density (g/cm³)')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Density Distribution')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Generated Materials Table\n",
    "ax = axes[1, 2]\n",
    "ax.axis('off')\n",
    "table_data = generated_df.head(8)[['formula', 'melting_point', 'density']].copy()\n",
    "table_data['melting_point'] = table_data['melting_point'].round(1)\n",
    "table_data['density'] = table_data['density'].round(3)\n",
    "table = ax.table(cellText=table_data.values, colLabels=table_data.columns, \n",
    "                 cellLoc='center', loc='center', bbox=[0, 0, 1, 1])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(9)\n",
    "table.scale(1, 1.5)\n",
    "ax.set_title('Sample Generated Materials', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('materials_discovery_workshop.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualization saved as 'materials_discovery_workshop.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Export Results\n",
    "\n",
    "Let's save our generated materials for further analysis or fabrication testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save generated materials\n",
    "generated_df.to_csv('generated_materials_workshop.csv', index=False)\n",
    "print(\"Generated materials saved to 'generated_materials_workshop.csv'\")\n",
    "\n",
    "# Workshop summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MATERIALS DISCOVERY WORKSHOP SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Original materials analyzed: {len(binary_data)}\")\n",
    "print(f\"New materials generated: {len(generated_df)}\")\n",
    "print(f\"Material clusters identified: {optimal_clusters}\")\n",
    "print()\n",
    "print(\"Key Takeaways:\")\n",
    "print(\"- ML can learn complex patterns in materials data\")\n",
    "print(\"- VAE models can generate novel material compositions\")\n",
    "print(\"- Clustering reveals natural groupings in materials space\")\n",
    "print(\"- This approach can accelerate materials R&D workflows\")\n",
    "print()\n",
    "print(\"Next Steps:\")\n",
    "print(\"- Validate generated materials experimentally\")\n",
    "print(\"- Extend to ternary and higher-order alloys\")\n",
    "print(\"- Incorporate additional material properties\")\n",
    "print(\"- Use reinforcement learning for property optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Advanced Validation Techniques\n",
    "\n",
    "For production-ready material discovery, we implement comprehensive validation techniques including cross-validation stability testing, baseline model comparisons, robustness testing, and domain-specific validation using materials science principles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced validation techniques for material discovery\n",
    "\n",
    "print(\"=== ADVANCED VALIDATION TECHNIQUES ===\")\n",
    "\n",
    "# 1. Cross-validation stability test\n",
    "print(\"\\n1. CROSS-VALIDATION STABILITY TEST\")\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import ks_2samp\n",
    "import numpy as np\n",
    "\n",
    "def cross_validate_vae(features_scaled, latent_dim=5, epochs=20, folds=3):\n",
    "    \"\"\"Test VAE stability across different data splits\"\"\"\n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "    cv_losses = []\n",
    "    cv_generation_diversity = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(features_scaled)):\n",
    "        print(f\"  Training fold {fold+1}/{folds}...\")\n",
    "        \n",
    "        # Split data\n",
    "        train_data = features_scaled[train_idx]\n",
    "        val_data = features_scaled[val_idx]\n",
    "        \n",
    "        # Quick training on this fold\n",
    "        model_fold = OptimizedVAE(input_dim=features_scaled.shape[1], latent_dim=latent_dim).to(device)\n",
    "        optimizer = optim.Adam(model_fold.parameters(), lr=0.005)\n",
    "        \n",
    "        train_tensor = torch.FloatTensor(train_data)\n",
    "        val_tensor = torch.FloatTensor(val_data)\n",
    "        \n",
    "        model_fold.train()\n",
    "        for epoch in range(epochs):\n",
    "            reconstructed, mu, log_var = model_fold(train_tensor.to(device))\n",
    "            reconstruction_loss = nn.functional.mse_loss(reconstructed, train_tensor.to(device), reduction='sum')\n",
    "            kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "            kl_weight = min(1.0, epoch / 10.0)\n",
    "            loss = reconstruction_loss + kl_weight * kl_loss\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Evaluate reconstruction on validation set\n",
    "        model_fold.eval()\n",
    "        with torch.no_grad():\n",
    "            val_reconstructed, _, _ = model_fold(val_tensor.to(device))\n",
    "            val_loss = nn.functional.mse_loss(val_reconstructed, val_tensor.to(device))\n",
    "            cv_losses.append(val_loss.item())\n",
    "            \n",
    "            # Test generation diversity\n",
    "            z_test = torch.randn(100, latent_dim).to(device)\n",
    "            generated_test = model_fold.decode(z_test).cpu().numpy()\n",
    "            generated_test = scaler.inverse_transform(generated_test)\n",
    "            cv_generation_diversity.append(np.std(generated_test, axis=0).mean())\n",
    "    \n",
    "    print(f\"Cross-validation reconstruction MSE: {np.mean(cv_losses):.4f} ± {np.std(cv_losses):.4f}\")\n",
    "    print(f\"Generation diversity stability: {np.mean(cv_generation_diversity):.4f} ± {np.std(cv_generation_diversity):.4f}\")\n",
    "    \n",
    "    return cv_losses, cv_generation_diversity\n",
    "\n",
    "# Run cross-validation\n",
    "cv_losses, cv_diversity = cross_validate_vae(features_scaled, epochs=10, folds=3)\n",
    "\n",
    "# 2. Baseline model comparison\n",
    "print(\"\\n2. BASELINE MODEL COMPARISON\")\n",
    "\n",
    "def comprehensive_baseline_comparison(features_scaled, binary_data):\n",
    "    \"\"\"Compare VAE against multiple baseline approaches\"\"\"\n",
    "    \n",
    "    # PCA baseline\n",
    "    pca = PCA(n_components=5)\n",
    "    pca_reconstructed = pca.inverse_transform(pca.fit_transform(features_scaled))\n",
    "    pca_mse = np.mean((features_scaled - pca_reconstructed) ** 2)\n",
    "    pca_explained_var = pca.explained_variance_ratio_.sum()\n",
    "    \n",
    "    # Random Forest baseline\n",
    "    rf_predictions = []\n",
    "    for i, col in enumerate(['melting_point', 'density', 'electronegativity', 'atomic_radius']):\n",
    "        feature_idx = feature_cols.index(col)\n",
    "        X = np.delete(features_scaled, feature_idx, axis=1)\n",
    "        y = features_scaled[:, feature_idx]\n",
    "        \n",
    "        rf = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "        rf.fit(X, y)\n",
    "        pred = rf.predict(X)\n",
    "        rf_predictions.append(pred)\n",
    "    \n",
    "    rf_reconstructed = features_scaled.copy()\n",
    "    for i in range(4):\n",
    "        feature_idx = feature_cols.index(['melting_point', 'density', 'electronegativity', 'atomic_radius'][i])\n",
    "        rf_reconstructed[:, feature_idx] = rf_predictions[i]\n",
    "    \n",
    "    rf_mse = np.mean((features_scaled - rf_reconstructed) ** 2)\n",
    "    \n",
    "    # VAE reconstruction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        vae_reconstructed, _, _ = model(features_tensor.to(device))\n",
    "        vae_reconstructed = vae_reconstructed.cpu().numpy()\n",
    "        vae_mse = np.mean((features_scaled - vae_reconstructed) ** 2)\n",
    "    \n",
    "    print(f\"PCA Reconstruction MSE: {pca_mse:.4f} (explained variance: {pca_explained_var:.3f})\")\n",
    "    print(f\"Random Forest Reconstruction MSE: {rf_mse:.4f}\")\n",
    "    print(f\"VAE Reconstruction MSE: {vae_mse:.4f}\")\n",
    "    print(f\"VAE improvement over PCA: {(pca_mse - vae_mse) / pca_mse * 100:.1f}%\")\n",
    "    print(f\"VAE improvement over RF: {(rf_mse - vae_mse) / rf_mse * 100:.1f}%\")\n",
    "    \n",
    "    # Generation capability comparison\n",
    "    print(\"\\nGeneration Capability Analysis:\")\n",
    "    \n",
    "    # PCA can only interpolate, not extrapolate\n",
    "    pca_min, pca_max = np.min(pca_reconstructed, axis=0), np.max(pca_reconstructed, axis=0)\n",
    "    original_min, original_max = np.min(features_scaled, axis=0), np.max(features_scaled, axis=0)\n",
    "    pca_coverage = np.mean((pca_max - pca_min) / (original_max - original_min))\n",
    "    \n",
    "    # VAE generation range\n",
    "    large_sample_size = 1000\n",
    "    with torch.no_grad():\n",
    "        z_large = torch.randn(large_sample_size, model.latent_dim).to(device)\n",
    "        vae_generated = model.decode(z_large).cpu().numpy()\n",
    "    \n",
    "    vae_min, vae_max = np.min(vae_generated, axis=0), np.max(vae_generated, axis=0)\n",
    "    vae_coverage = np.mean((vae_max - vae_min) / (original_max - original_min))\n",
    "    \n",
    "    print(f\"PCA generation coverage (relative to training range): {pca_coverage:.3f}\")\n",
    "    print(f\"VAE generation coverage (relative to training range): {vae_coverage:.3f}\")\n",
    "    print(f\"VAE expands generation space by {vae_coverage/pca_coverage:.1f}x compared to PCA\")\n",
    "    \n",
    "    return {\n",
    "        'pca_mse': pca_mse, 'rf_mse': rf_mse, 'vae_mse': vae_mse,\n",
    "        'pca_coverage': pca_coverage, 'vae_coverage': vae_coverage\n",
    "    }\n",
    "\n",
    "baseline_results = comprehensive_baseline_comparison(features_scaled, binary_data)\n",
    "\n",
    "# 3. Random seed robustness testing\n",
    "print(\"\\n3. RANDOM SEED ROBUSTNESS TEST\")\n",
    "\n",
    "def test_random_seeds_comprehensive(features_scaled, seeds=[42, 123, 456, 789, 999]):\n",
    "    \"\"\"Comprehensive robustness testing across different random seeds\"\"\"\n",
    "    seed_results = []\n",
    "    \n",
    "    for seed in seeds:\n",
    "        print(f\"  Testing seed {seed}...\")\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        model_seed = OptimizedVAE(input_dim=features_scaled.shape[1], latent_dim=5).to(device)\n",
    "        optimizer = optim.Adam(model_seed.parameters(), lr=0.005)\n",
    "        \n",
    "        model_seed.train()\n",
    "        for epoch in range(15):  # Quick training\n",
    "            reconstructed, mu, log_var = model_seed(features_tensor.to(device))\n",
    "            reconstruction_loss = nn.functional.mse_loss(reconstructed, features_tensor.to(device), reduction='sum')\n",
    "            kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "            kl_weight = min(1.0, epoch / 10.0)\n",
    "            loss = reconstruction_loss + kl_weight * kl_loss\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Evaluate multiple metrics\n",
    "        model_seed.eval()\n",
    "        with torch.no_grad():\n",
    "            final_reconstructed, _, _ = model_seed(features_tensor.to(device))\n",
    "            reconstruction_mse = nn.functional.mse_loss(final_reconstructed, features_tensor.to(device))\n",
    "            \n",
    "            # Generation diversity\n",
    "            z_test = torch.randn(200, model_seed.latent_dim).to(device)\n",
    "            generated_test = model_seed.decode(z_test).cpu().numpy()\n",
    "            generation_std = np.std(generated_test, axis=0).mean()\n",
    "            \n",
    "            # Latent space coverage\n",
    "            _, mu_test, log_var_test = model_seed(features_tensor.to(device))\n",
    "            latent_coverage = np.std(mu_test.cpu().numpy(), axis=0).mean()\n",
    "        \n",
    "        seed_results.append({\n",
    "            'seed': seed,\n",
    "            'reconstruction_mse': reconstruction_mse.item(),\n",
    "            'generation_diversity': generation_std,\n",
    "            'latent_coverage': latent_coverage\n",
    "        })\n",
    "    \n",
    "    # Analyze robustness\n",
    "    mse_values = [r['reconstruction_mse'] for r in seed_results]\n",
    "    diversity_values = [r['generation_diversity'] for r in seed_results]\n",
    "    coverage_values = [r['latent_coverage'] for r in seed_results]\n",
    "    \n",
    "    print(f\"Reconstruction MSE across seeds: {np.mean(mse_values):.4f} ± {np.std(mse_values):.4f}\")\n",
    "    print(f\"Generation diversity across seeds: {np.mean(diversity_values):.4f} ± {np.std(diversity_values):.4f}\")\n",
    "    print(f\"Latent coverage across seeds: {np.mean(coverage_values):.4f} ± {np.std(coverage_values):.4f}\")\n",
    "    \n",
    "    # Coefficient of variation (lower is better)\n",
    "    mse_cv = np.std(mse_values) / np.mean(mse_values)\n",
    "    diversity_cv = np.std(diversity_values) / np.mean(diversity_values)\n",
    "    coverage_cv = np.std(coverage_values) / np.mean(coverage_values)\n",
    "    \n",
    "    print(f\"Reconstruction stability (CV): {mse_cv:.3f}\")\n",
    "    print(f\"Generation stability (CV): {diversity_cv:.3f}\")\n",
    "    print(f\"Latent space stability (CV): {coverage_cv:.3f}\")\n",
    "    \n",
    "    if mse_cv < 0.1 and diversity_cv < 0.1:\n",
    "        print(\"✓ Model shows good robustness across random seeds\")\n",
    "    else:\n",
    "        print(\"⚠ Model shows some variability across random seeds\")\n",
    "    \n",
    "    return seed_results\n",
    "\n",
    "seed_robustness_results = test_random_seeds_comprehensive(features_scaled)\n",
    "\n",
    "# 4. Domain-specific materials science validation\n",
    "print(\"\\n4. DOMAIN-SPECIFIC MATERIALS SCIENCE VALIDATION\")\n",
    "\n",
    "# Enhanced Hume-Rothery rules check with actual element data\n",
    "def advanced_hume_rothery_validation(generated_materials, elements_list):\n",
    "    \"\"\"Advanced Hume-Rothery rules validation with realistic element properties\"\"\"\n",
    "    \n",
    "    # Element properties database (simplified but realistic)\n",
    "    element_properties = {\n",
    "        'Al': {'atomic_radius': 1.43, 'electronegativity': 1.61, 'valence': 3, 'crystal_structure': 'FCC'},\n",
    "        'Ti': {'atomic_radius': 1.47, 'electronegativity': 1.54, 'valence': 4, 'crystal_structure': 'HCP'},\n",
    "        'V': {'atomic_radius': 1.34, 'electronegativity': 1.63, 'valence': 5, 'crystal_structure': 'BCC'},\n",
    "        'Cr': {'atomic_radius': 1.28, 'electronegativity': 1.66, 'valence': 6, 'crystal_structure': 'BCC'},\n",
    "        'Mn': {'atomic_radius': 1.27, 'electronegativity': 1.55, 'valence': 7, 'crystal_structure': 'BCC'},\n",
    "        'Fe': {'atomic_radius': 1.26, 'electronegativity': 1.83, 'valence': 2, 'crystal_structure': 'BCC'},\n",
    "        'Co': {'atomic_radius': 1.25, 'electronegativity': 1.88, 'valence': 2, 'crystal_structure': 'HCP'},\n",
    "        'Ni': {'atomic_radius': 1.25, 'electronegativity': 1.91, 'valence': 2, 'crystal_structure': 'FCC'},\n",
    "        'Cu': {'atomic_radius': 1.28, 'electronegativity': 1.90, 'valence': 1, 'crystal_structure': 'FCC'},\n",
    "        'Zn': {'atomic_radius': 1.37, 'electronegativity': 1.65, 'valence': 2, 'crystal_structure': 'HCP'}\n",
    "    }\n",
    "    \n",
    "    violations = {'total': 0, 'radius': 0, 'electronegativity': 0, 'valence': 0, 'structure': 0}\n",
    "    valid_compositions = []\n",
    "    \n",
    "    for _, material in generated_materials.iterrows():\n",
    "        elem1, elem2 = material['element_1'], material['element_2']\n",
    "        \n",
    "        if elem1 not in element_properties or elem2 not in element_properties:\n",
    "            continue\n",
    "            \n",
    "        props1 = element_properties[elem1]\n",
    "        props2 = element_properties[elem2]\n",
    "        \n",
    "        composition_valid = True\n",
    "        \n",
    "        # Rule 1: Atomic radius difference < 15%\n",
    "        radius_diff = abs(props1['atomic_radius'] - props2['atomic_radius']) / max(props1['atomic_radius'], props2['atomic_radius'])\n",
    "        if radius_diff > 0.15:\n",
    "            violations['radius'] += 1\n",
    "            composition_valid = False\n",
    "        \n",
    "        # Rule 2: Similar electronegativity (< 0.4 difference)\n",
    "        electroneg_diff = abs(props1['electronegativity'] - props2['electronegativity'])\n",
    "        if electroneg_diff > 0.4:\n",
    "            violations['electronegativity'] += 1\n",
    "            composition_valid = False\n",
    "        \n",
    "        # Rule 3: Similar valence\n",
    "        if abs(props1['valence'] - props2['valence']) > 1:\n",
    "            violations['valence'] += 1\n",
    "            composition_valid = False\n",
    "        \n",
    "        # Rule 4: Compatible crystal structures (simplified)\n",
    "        if props1['crystal_structure'] != props2['crystal_structure']:\n",
    "            # Allow FCC-HCP, BCC-HCP, but penalize FCC-BCC\n",
    "            if {props1['crystal_structure'], props2['crystal_structure']} == {'FCC', 'BCC'}:\n",
    "                violations['structure'] += 1\n",
    "                composition_valid = False\n",
    "        \n",
    "        if composition_valid:\n",
    "            valid_compositions.append(material)\n",
    "        else:\n",
    "            violations['total'] += 1\n",
    "    \n",
    "    total_materials = len(generated_materials)\n",
    "    valid_percentage = len(valid_compositions) / total_materials * 100\n",
    "    \n",
    "    print(f\"Hume-Rothery Rules Validation Results:\")\n",
    "    print(f\"Total materials evaluated: {total_materials}\")\n",
    "    print(f\"Valid compositions: {len(valid_compositions)} ({valid_percentage:.1f}%)\")\n",
    "    print(f\"Violations by rule:\")\n",
    "    print(f\"  - Atomic radius difference: {violations['radius']} ({violations['radius']/total_materials*100:.1f}%)\")\n",
    "    print(f\"  - Electronegativity difference: {violations['electronegativity']} ({violations['electronegativity']/total_materials*100:.1f}%)\")\n",
    "    print(f\"  - Valence difference: {violations['valence']} ({violations['valence']/total_materials*100:.1f}%)\")\n",
    "    print(f\"  - Crystal structure: {violations['structure']} ({violations['structure']/total_materials*100:.1f}%)\")\n",
    "    \n",
    "    if valid_percentage > 70:\n",
    "        print(\"✓ Good compliance with Hume-Rothery rules\")\n",
    "    elif valid_percentage > 50:\n",
    "        print(\"⚠ Moderate compliance with Hume-Rothery rules\")\n",
    "    else:\n",
    "        print(\"⚠ Low compliance with Hume-Rothery rules - review generation constraints\")\n",
    "    \n",
    "    return valid_compositions, violations\n",
    "\n",
    "# Run domain validation\n",
    "valid_materials, hume_violations = advanced_hume_rothery_validation(generated_df, elements)\n",
    "\n",
    "# 5. Statistical robustness testing\n",
    "print(\"\\n5. STATISTICAL ROBUSTNESS TESTING\")\n",
    "\n",
    "def statistical_robustness_analysis(original_data, generated_data, n_bootstraps=100):\n",
    "    \"\"\"Statistical analysis of generation robustness using bootstrapping\"\"\"\n",
    "    \n",
    "    properties = ['melting_point', 'density', 'electronegativity', 'atomic_radius']\n",
    "    robustness_scores = {}\n",
    "    \n",
    "    print(f\"Running {n_bootstraps} bootstrap samples for statistical robustness...\")\n",
    "    \n",
    "    for prop in properties:\n",
    "        original_vals = original_data[prop].values\n",
    "        generated_vals = generated_data[prop].values\n",
    "        \n",
    "        # Bootstrap KS test p-values\n",
    "        ks_p_values = []\n",
    "        \n",
    "        for _ in range(n_bootstraps):\n",
    "            # Bootstrap sample from generated data\n",
    "            bootstrap_idx = np.random.choice(len(generated_vals), len(generated_vals), replace=True)\n",
    "            bootstrap_generated = generated_vals[bootstrap_idx]\n",
    "            \n",
    "            # KS test\n",
    "            _, p_value = ks_2samp(original_vals, bootstrap_generated)\n",
    "            ks_p_values.append(p_value)\n",
    "        \n",
    "        # Robustness metrics\n",
    "        mean_p = np.mean(ks_p_values)\n",
    "        std_p = np.std(ks_p_values)\n",
    "        p_stability = 1 - std_p  # Lower variance = higher stability\n",
    "        \n",
    "        # Confidence interval for p-value\n",
    "        ci_lower = np.percentile(ks_p_values, 2.5)\n",
    "        ci_upper = np.percentile(ks_p_values, 97.5)\n",
    "        \n",
    "        robustness_scores[prop] = {\n",
    "            'mean_p_value': mean_p,\n",
    "            'p_stability': p_stability,\n",
    "            'ci_95': (ci_lower, ci_upper),\n",
    "            'distribution_similarity': 'good' if mean_p > 0.05 else 'poor'\n",
    "        }\n",
    "        \n",
    "        print(f\"{prop}:\")\n",
    "        print(f\"  Mean KS p-value: {mean_p:.4f}\")\n",
    "        print(f\"  P-value stability: {p_stability:.4f}\")\n",
    "        print(f\"  95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
    "        print(f\"  Distribution match: {robustness_scores[prop]['distribution_similarity']}\")\n",
    "    \n",
    "    # Overall robustness score\n",
    "    avg_stability = np.mean([scores['p_stability'] for scores in robustness_scores.values()])\n",
    "    good_matches = sum(1 for scores in robustness_scores.values() if scores['distribution_similarity'] == 'good')\n",
    "    \n",
    "    print(f\"\\nOverall robustness assessment:\")\n",
    "    print(f\"Average p-value stability: {avg_stability:.4f}\")\n",
    "    print(f\"Properties with good distribution match: {good_matches}/{len(properties)}\")\n",
    "    \n",
    "    if avg_stability > 0.8 and good_matches >= 3:\n",
    "        print(\"✓ High statistical robustness - generation is reliable\")\n",
    "    elif avg_stability > 0.6 and good_matches >= 2:\n",
    "        print(\"⚠ Moderate statistical robustness - acceptable for exploratory work\")\n",
    "    else:\n",
    "        print(\"⚠ Low statistical robustness - consider model improvements\")\n",
    "    \n",
    "    return robustness_scores\n",
    "\n",
    "# Run statistical robustness testing\n",
    "statistical_results = statistical_robustness_analysis(binary_data, generated_df)\n",
    "\n",
    "# 6. Model evaluation metrics and benchmarking\n",
    "print(\"\\n6. COMPREHENSIVE MODEL EVALUATION METRICS\")\n",
    "\n",
    "def comprehensive_model_evaluation(model, features_scaled, generated_df, binary_data):\n",
    "    \"\"\"Comprehensive evaluation metrics for generative model assessment\"\"\"\n",
    "    \n",
    "    evaluation_metrics = {}\n",
    "    \n",
    "    # Reconstruction quality\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        reconstructed, _, _ = model(features_tensor.to(device))\n",
    "        reconstruction_mse = nn.functional.mse_loss(reconstructed, features_tensor.to(device))\n",
    "        reconstruction_rmse = torch.sqrt(reconstruction_mse)\n",
    "    \n",
    "    evaluation_metrics['reconstruction'] = {\n",
    "        'mse': reconstruction_mse.item(),\n",
    "        'rmse': reconstruction_rmse.item(),\n",
    "        'quality': 'excellent' if reconstruction_mse.item() < 0.1 else 'good' if reconstruction_mse.item() < 0.5 else 'poor'\n",
    "    }\n",
    "    \n",
    "    # Generation diversity\n",
    "    large_sample_size = 1000\n",
    "    with torch.no_grad():\n",
    "        z_large = torch.randn(large_sample_size, model.latent_dim).to(device)\n",
    "        generated_large = model.decode(z_large).cpu().numpy()\n",
    "        generated_large = scaler.inverse_transform(generated_large)\n",
    "    \n",
    "    generation_std = np.std(generated_large, axis=0)\n",
    "    diversity_score = np.mean(generation_std)\n",
    "    \n",
    "    evaluation_metrics['diversity'] = {\n",
    "        'mean_std': diversity_score,\n",
    "        'range_coverage': np.mean((np.max(generated_large, axis=0) - np.min(generated_large, axis=0)) / \n",
    "                                  (np.max(features_scaled, axis=0) - np.min(features_scaled, axis=0))),\n",
    "        'quality': 'high' if diversity_score > 0.5 else 'moderate' if diversity_score > 0.3 else 'low'\n",
    "    }\n",
    "    \n",
    "    # Latent space analysis\n",
    "    with torch.no_grad():\n",
    "        _, mu, log_var = model(features_tensor.to(device))\n",
    "        mu_np = mu.cpu().numpy()\n",
    "        log_var_np = log_var.cpu().numpy()\n",
    "    \n",
    "    evaluation_metrics['latent_space'] = {\n",
    "        'dimension': model.latent_dim,\n",
    "        'mu_mean': np.mean(mu_np),\n",
    "        'mu_std': np.std(mu_np),\n",
    "        'log_var_mean': np.mean(log_var_np),\n",
    "        'log_var_std': np.std(log_var_np),\n",
    "        'regularity': 'good' if abs(np.mean(log_var_np)) < 1.0 else 'poor'\n",
    "    }\n",
    "    \n",
    "    # Training stability\n",
    "    evaluation_metrics['training'] = {\n",
    "        'final_loss': losses[-1],\n",
    "        'convergence': 'good' if losses[-1] < losses[0] * 0.1 else 'moderate' if losses[-1] < losses[0] * 0.5 else 'poor',\n",
    "        'epochs': len(losses)\n",
    "    }\n",
    "    \n",
    "    # Print comprehensive evaluation\n",
    "    print(\"COMPREHENSIVE MODEL EVALUATION:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(\"\\nReconstruction Quality:\")\n",
    "    rec = evaluation_metrics['reconstruction']\n",
    "    print(f\"  MSE: {rec['mse']:.6f}, RMSE: {rec['rmse']:.6f} ({rec['quality']})\")\n",
    "    \n",
    "    print(\"\\nGeneration Diversity:\")\n",
    "    div = evaluation_metrics['diversity']\n",
    "    print(f\"  Mean std: {div['mean_std']:.4f}, Range coverage: {div['range_coverage']:.3f} ({div['quality']})\")\n",
    "    \n",
    "    print(\"\\nLatent Space Analysis:\")\n",
    "    lat = evaluation_metrics['latent_space']\n",
    "    print(f\"  Dimension: {lat['dimension']}, μ mean: {lat['mu_mean']:.3f}, logσ² mean: {lat['log_var_mean']:.3f} ({lat['regularity']})\")\n",
    "    \n",
    "    print(\"\\nTraining Stability:\")\n",
    "    tr = evaluation_metrics['training']\n",
    "    print(f\"  Final loss: {tr['final_loss']:.6f}, Convergence: {tr['convergence']} ({tr['epochs']} epochs)\")\n",
    "    \n",
    "    # Overall assessment\n",
    "    quality_scores = {\n",
    "        'excellent': 3, 'good': 2, 'moderate': 1, 'high': 3, 'low': 0, 'poor': 0\n",
    "    }\n",
    "    \n",
    "    overall_score = (\n",
    "        quality_scores.get(rec['quality'], 1) +\n",
    "        quality_scores.get(div['quality'], 1) +\n",
    "        (3 if lat['regularity'] == 'good' else 1) +\n",
    "        quality_scores.get(tr['convergence'], 1)\n",
    "    ) / 4\n",
    "    \n",
    "    if overall_score >= 2.5:\n",
    "        print(f\"\\n🎉 OVERALL ASSESSMENT: EXCELLENT MODEL (score: {overall_score:.2f}/3)\")\n",
    "    elif overall_score >= 1.5:\n",
    "        print(f\"\\n✅ OVERALL ASSESSMENT: GOOD MODEL (score: {overall_score:.2f}/3)\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️ OVERALL ASSESSMENT: NEEDS IMPROVEMENT (score: {overall_score:.2f}/3)\")\n",
    "    \n",
    "    return evaluation_metrics\n",
    "\n",
    "# Run comprehensive evaluation\n",
    "evaluation_results = comprehensive_model_evaluation(model, features_scaled, generated_df, binary_data)\n",
    "\n",
    "# Final validation summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ADVANCED VALIDATION TECHNIQUES - FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"✓ Cross-validation stability testing completed\")\n",
    "print(\"✓ Baseline model comparison (PCA, Random Forest) done\")\n",
    "print(\"✓ Random seed robustness testing completed\")\n",
    "print(\"✓ Domain-specific validation (Hume-Rothery rules) completed\")\n",
    "print(\"✓ Statistical robustness testing (Kolmogorov-Smirnov) completed\")\n",
    "print(\"✓ Comprehensive model evaluation metrics calculated\")\n",
    "\n",
    "print(\"\\nKey Findings:\")\n",
    "print(f\"- Model shows {evaluation_results['reconstruction']['quality']} reconstruction quality\")\n",
    "print(f\"- Generation diversity is {evaluation_results['diversity']['quality']}\")\n",
    "print(f\"- Cross-validation stability: {np.std(cv_losses)/np.mean(cv_losses):.3f} CV\")\n",
    "print(f\"- Valid Hume-Rothery compositions: {len(valid_materials)}/{len(generated_df)} ({len(valid_materials)/len(generated_df)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nRecommendations for Production Use:\")\n",
    "if evaluation_results['reconstruction']['quality'] == 'excellent' and len(valid_materials)/len(generated_df) > 0.7:\n",
    "    print(\"🎯 MODEL READY FOR PRODUCTION - High confidence in generated materials\")\n",
    "elif evaluation_results['reconstruction']['quality'] in ['good', 'excellent'] and len(valid_materials)/len(generated_df) > 0.5:\n",
    "    print(\"⚠️ MODEL SUITABLE FOR EXPLORATORY WORK - Validate generated materials experimentally\")\n",
    "else:\n",
    "    print(\"🔄 MODEL NEEDS IMPROVEMENT - Consider architecture changes or additional training\")\n",
    "\n",
    "print(\"\\nAdvanced validation completed successfully!\")\n",
    "print(\"The enhanced workshop now provides comprehensive validation for production-ready material discovery.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
