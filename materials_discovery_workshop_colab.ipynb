{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "materials-discovery-workshop"
   },
   "source": [
    "# Materials Discovery Workshop - Google Colab Edition\n",
    "\n",
    "This interactive notebook demonstrates how machine learning can accelerate materials discovery by learning patterns from existing alloy compositions and generating new ones.\n",
    "\n",
    "**Workshop Goals:**\n",
    "- Understand how variational autoencoders (VAEs) can model materials data\n",
    "- Learn to generate new alloy compositions using ML\n",
    "- Explore materials clustering and property analysis\n",
    "- See how AI can accelerate materials R&D\n",
    "\n",
    "**What you'll need:**\n",
    "- Basic understanding of alloys and material properties\n",
    "- Curiosity about how ML can help with materials science\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-and-data-loading"
   },
   "source": [
    "## Step 1: Setup and Data Loading\n",
    "\n",
    "First, let's install dependencies and load our materials dataset. This dataset contains alloy compositions and their properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-dependencies"
   },
   "outputs": [],
   "source": [
    "# Install required packages for Colab\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install scikit-learn matplotlib seaborn pandas numpy ipywidgets pymatgen requests\n",
    "\n",
    "print(\"\u2705 Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import-libraries"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"\u00e2\u0153\u2026 Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Running on: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download-dataset"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from scipy.stats import ks_2samp\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"\u2705 Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Running on: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data-source-selection"
   },
   "source": [
    "## \ud83c\udd95 NEW: Choose Your Data Source\n",
    "\n",
    "This workshop now supports two data sources:\n",
    "\n",
    "1. **Synthetic Data** (Original): Programmatically generated for demonstrations\n",
    "2. **Materials Project Data** (NEW): Real materials from computational database\n",
    "\n",
    "Choose your data source below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data-source-choice"
   },
   "outputs": [],
   "source": [
    "# Data source selection\n",
    "data_source = widgets.Dropdown(\n",
    "    options=['Synthetic (Demo)', 'Materials Project (Real)'],\n",
    "    value='Materials Project (Real)',\n",
    "    description='Data Source:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "display(data_source)\n",
    "\n",
    "print(\"\\n\ud83c\udfaf Selected data source will be loaded in the next cell.\")\n",
    "print(\"   - Synthetic: Fast, good for learning concepts\")\n",
    "print(\"   - Materials Project: Real data, production-ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-materials-data"
   },
   "outputs": [],
   "source": [
    "# Materials Project API Integration Class\n",
    "class MaterialsProjectClient:\n",
    "    \"\"\"Client for Materials Project API with rate limiting and error handling.\"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str = \"pkHkQjeWQe8lFY29NV2p1yQ52rBKX3KE\"):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"https://api.materialsproject.org\"\n",
    "        self.last_request_time = 0\n",
    "        self.rate_limit_delay = 0.2\n",
    "        self.max_retries = 3\n",
    "\n",
    "    def _rate_limit_wait(self):\n",
    "        current_time = time.time()\n",
    "        time_since_last = current_time - self.last_request_time\n",
    "        if time_since_last < self.rate_limit_delay:\n",
    "            time.sleep(self.rate_limit_delay - time_since_last)\n",
    "        self.last_request_time = time.time()\n",
    "\n",
    "    def _make_request(self, endpoint: str, params: Dict = None) -> Dict:\n",
    "        if params is None:\n",
    "            params = {}\n",
    "        headers = {\"X-API-Key\": self.api_key}\n",
    "\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                self._rate_limit_wait()\n",
    "                response = requests.get(f\"{self.base_url}{endpoint}\", params=params, headers=headers, timeout=30)\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    return response.json()\n",
    "                elif response.status_code == 429:\n",
    "                    time.sleep(5)\n",
    "                    continue\n",
    "                else:\n",
    "                    if attempt < self.max_retries - 1:\n",
    "                        time.sleep(2 ** attempt)\n",
    "                        continue\n",
    "                    raise Exception(f\"API error {response.status_code}\")\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                if attempt < self.max_retries - 1:\n",
    "                    time.sleep(2 ** attempt)\n",
    "                    continue\n",
    "                raise\n",
    "\n",
    "        raise Exception(\"All API attempts failed\")\n",
    "\n",
    "    def get_materials_summary(self, elements: List[str] = None, limit: int = 100) -> pd.DataFrame:\n",
    "        params = {\n",
    "            \"_fields\": \"material_id,formula_pretty,elements,nsites,volume,density,density_atomic,band_gap,energy_above_hull,formation_energy_per_atom,total_magnetization\",\n",
    "            \"_limit\": limit\n",
    "        }\n",
    "\n",
    "        if elements:\n",
    "            params[\"elements\"] = \",\".join(elements)\n",
    "\n",
    "        response = self._make_request(\"/materials/summary/\", params)\n",
    "        materials = response.get(\"data\", [])\n",
    "\n",
    "        if not materials:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        df = pd.DataFrame(materials)\n",
    "        df.rename(columns={'formula_pretty': 'formula'}, inplace=True)\n",
    "\n",
    "        numeric_cols = ['nsites', 'volume', 'density', 'atomic_density', 'band_gap', 'energy_above_hull', 'formation_energy_per_atom', 'total_magnetization']\n",
    "        for col in numeric_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "        return df\n",
    "\n",
    "    def get_binary_alloys(self, element_pairs: List[Tuple[str, str]] = None, limit_per_pair: int = 50) -> pd.DataFrame:\n",
    "        if element_pairs is None:\n",
    "            element_pairs = [('Al', 'Ti'), ('Al', 'V'), ('Al', 'Cr'), ('Al', 'Fe'), ('Al', 'Ni'), ('Al', 'Cu'),\n",
    "                            ('Ti', 'V'), ('Ti', 'Cr'), ('Ti', 'Fe'), ('Ti', 'Ni'), ('V', 'Cr'), ('Fe', 'Co'), ('Fe', 'Ni'), ('Co', 'Ni'), ('Ni', 'Cu')]\n",
    "\n",
    "        all_materials = []\n",
    "        for elem1, elem2 in element_pairs:\n",
    "            materials = self.get_materials_summary(elements=[elem1, elem2], limit=limit_per_pair)\n",
    "            if not materials.empty:\n",
    "                materials['element_1'] = elem1\n",
    "                materials['element_2'] = elem2\n",
    "                materials['alloy_type'] = 'binary'\n",
    "                all_materials.append(materials)\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        if not all_materials:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        combined_df = pd.concat(all_materials, ignore_index=True)\n",
    "        combined_df.drop_duplicates(subset='material_id', inplace=True)\n",
    "        return combined_df\n",
    "\n",
    "# Load selected data source\n",
    "if data_source.value == 'Materials Project (Real)':\n",
    "    print(\"\ud83d\udd04 Loading REAL materials data from Materials Project...\")\n",
    "    \n",
    "    try:\n",
    "        client = MaterialsProjectClient()\n",
    "        \n",
    "        # Test connection\n",
    "        test_data = client.get_materials_summary(elements=[\"Al\", \"Ti\"], limit=5)\n",
    "        if test_data.empty:\n",
    "            raise Exception(\"API connection failed\")\n",
    "        \n",
    "        # Get full dataset\n",
    "        raw_data = client.get_binary_alloys(limit_per_pair=30)\n",
    "        \n",
    "        if raw_data.empty:\n",
    "            raise Exception(\"No materials retrieved\")\n",
    "        \n",
    "        # Convert to ML features\n",
    "        import pymatgen.core as mg\n",
    "        \n",
    "        features_df = raw_data.copy()\n",
    "        for idx, row in features_df.iterrows():\n",
    "            if 'elements' in row and row['elements']:\n",
    "                elements = row['elements']\n",
    "                electronegativities = []\n",
    "                atomic_radii = []\n",
    "                \n",
    "                for elem_symbol in elements:\n",
    "                    try:\n",
    "                        elem = mg.Element(elem_symbol)\n",
    "                        if hasattr(elem, 'X') and elem.X is not None:\n",
    "                            electronegativities.append(elem.X)\n",
    "                        if hasattr(elem, 'atomic_radius') and elem.atomic_radius is not None:\n",
    "                            atomic_radii.append(elem.atomic_radius)\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                features_df.loc[idx, 'electronegativity'] = np.mean(electronegativities) if electronegativities else 0\n",
    "                features_df.loc[idx, 'atomic_radius'] = np.mean(atomic_radii) if atomic_radii else 0\n",
    "        \n",
    "        features_df['composition_1'] = 0.5\n",
    "        features_df['composition_2'] = 0.5\n",
    "        features_df['composition_3'] = 0.0\n",
    "        \n",
    "        ml_features = features_df[['composition_1', 'composition_2', 'composition_3', 'density', 'electronegativity', 'atomic_radius', 'band_gap', 'energy_above_hull', 'formation_energy_per_atom']].copy()\n",
    "        ml_features.rename(columns={'formation_energy_per_atom': 'melting_point'}, inplace=True)\n",
    "        ml_features.fillna(ml_features.mean(), inplace=True)\n",
    "        \n",
    "        ml_features['melting_point'] = ml_features['melting_point'].clip(-10, 10)\n",
    "        ml_features['density'] = ml_features['density'].clip(0, 50)\n",
    "        \n",
    "        data = features_df\n",
    "        data_type = \"real\"\n",
    "        \n",
    "        print(f\"\u2705 Loaded {len(ml_features)} REAL materials from Materials Project!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Failed to load Materials Project data: {e}\")\n",
    "        print(\"Falling back to synthetic data...\")\n",
    "        data_source.value = 'Synthetic (Demo)'\n",
    "\n",
    "if data_source.value == 'Synthetic (Demo)':\n",
    "    print(\"\ud83d\udd04 Creating SYNTHETIC materials dataset for demonstration...\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    \n",
    "    alloys = []\n",
    "    elements = ['Al', 'Ti', 'V', 'Cr', 'Mn', 'Fe', 'Co', 'Ni', 'Cu', 'Zn']\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        alloy_type = np.random.choice(['binary', 'ternary'], p=[0.7, 0.3])\n",
    "        \n",
    "        if alloy_type == 'binary':\n",
    "            elem1, elem2 = np.random.choice(elements, 2, replace=False)\n",
    "            comp1 = np.random.uniform(0.1, 0.9)\n",
    "            comp2 = 1 - comp1\n",
    "            comp3 = 0\n",
    "        else:\n",
    "            elem1, elem2, elem3 = np.random.choice(elements, 3, replace=False)\n",
    "            comp1 = np.random.uniform(0.1, 0.6)\n",
    "            comp2 = np.random.uniform(0.1, 0.6)\n",
    "            comp3 = 1 - comp1 - comp2\n",
    "        \n",
    "        melting_point = np.random.normal(1500, 300)\n",
    "        density = np.random.normal(7.8, 2.0)\n",
    "        electronegativity = np.random.normal(1.8, 0.3)\n",
    "        atomic_radius = np.random.normal(1.3, 0.2)\n",
    "        \n",
    "        alloys.append({\n",
    "            'id': f'alloy_{i+1}',\n",
    "            'alloy_type': alloy_type,\n",
    "            'element_1': elem1,\n",
    "            'element_2': elem2,\n",
    "            'element_3': elem3 if alloy_type == 'ternary' else None,\n",
    "            'composition_1': comp1,\n",
    "            'composition_2': comp2,\n",
    "            'composition_3': comp3,\n",
    "            'melting_point': max(500, melting_point),\n",
    "            'density': max(2, density),\n",
    "            'electronegativity': max(0.7, min(2.5, electronegativity)),\n",
    "            'atomic_radius': max(1.0, min(1.8, atomic_radius))\n",
    "        })\n",
    "    \n",
    "    data = pd.DataFrame(alloys)\n",
    "    data_type = \"synthetic\"\n",
    "    \n",
    "    # Create ML features from synthetic data\n",
    "    binary_data_synth = data[data['alloy_type'] == 'binary'].copy()\n",
    "    binary_data_synth['composition_3'] = binary_data_synth['composition_3'].fillna(0)\n",
    "    ml_features = binary_data_synth[['composition_1', 'composition_2', 'composition_3', 'melting_point', 'density', 'electronegativity', 'atomic_radius']].copy()\n",
    "    ml_features['band_gap'] = 0.0\n",
    "    ml_features['energy_above_hull'] = 0.0\n",
    "    \n",
    "    print(f\"\u2705 Created {len(ml_features)} SYNTHETIC materials for demonstration!\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Dataset ready: {len(ml_features)} materials ({data_type} data)\")\n",
    "print(\"First few rows:\")\n",
    "display_cols = ['alloy_type', 'element_1', 'element_2', 'density', 'melting_point'] if data_type == 'synthetic' else ['formula', 'elements', 'density', 'band_gap']\n",
    "print(data[display_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "interactive-parameters"
   },
   "source": [
    "## Interactive Parameters\n",
    "\n",
    "Let's set up some interactive controls to experiment with different model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "parameter-controls"
   },
   "outputs": [],
   "source": [
    "# Interactive parameter controls\n",
    "latent_dim_slider = widgets.IntSlider(value=5, min=2, max=20, step=1, description='Latent Dim:')\n",
    "epochs_slider = widgets.IntSlider(value=50, min=10, max=200, step=10, description='Epochs:')\n",
    "num_samples_slider = widgets.IntSlider(value=100, min=10, max=500, step=10, description='Samples:')\n",
    "\n",
    "display(latent_dim_slider, epochs_slider, num_samples_slider)\n",
    "\n",
    "# Global parameters (will be updated by widgets)\n",
    "params = {\n",
    "    'latent_dim': latent_dim_slider.value,\n",
    "    'epochs': epochs_slider.value,\n",
    "    'num_samples': num_samples_slider.value\n",
    "}\n",
    "\n",
    "def update_params(change):\n",
    "    params['latent_dim'] = latent_dim_slider.value\n",
    "    params['epochs'] = epochs_slider.value\n",
    "    params['num_samples'] = num_samples_slider.value\n",
    "    print(f\"Updated parameters: {params}\")\n",
    "\n",
    "latent_dim_slider.observe(update_params, names='value')\n",
    "epochs_slider.observe(update_params, names='value')\n",
    "num_samples_slider.observe(update_params, names='value')\n",
    "\n",
    "print(\"Interactive controls ready! Adjust the sliders and rerun cells below.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data-preprocessing"
   },
   "source": [
    "## Step 2: Data Preprocessing\n",
    "\n",
    "We need to prepare our data for machine learning. This involves:\n",
    "- Selecting relevant features\n",
    "- Handling missing values\n",
    "- Scaling the data\n",
    "\n",
    "Let's focus on binary alloys for this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "preprocess-data"
   },
   "outputs": [],
   "source": [
    "# Select binary alloys and key features\n",
    "binary_data = data[data['alloy_type'] == 'binary'].copy()\n",
    "binary_data['composition_3'] = binary_data['composition_3'].fillna(0)\n",
    "\n",
    "feature_cols = ['composition_1', 'composition_2', 'melting_point', \n",
    "               'density', 'electronegativity', 'atomic_radius']\n",
    "features = binary_data[feature_cols].values\n",
    "\n",
    "print(f\"Using {len(binary_data)} binary alloys\")\n",
    "print(f\"Feature matrix shape: {features.shape}\")\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "print(\"Features scaled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vae-architecture"
   },
   "source": [
    "## Step 3: The Variational Autoencoder (VAE)\n",
    "\n",
    "A VAE is a type of neural network that can learn to generate new data similar to its training data. Here's how it works:\n",
    "\n",
    "- **Encoder**: Compresses input data into a lower-dimensional latent space\n",
    "- **Latent Space**: A compressed representation where similar materials are close together\n",
    "- **Decoder**: Reconstructs data from the latent space\n",
    "\n",
    "The \"variational\" part means it learns a probability distribution, allowing us to sample new materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "define-vae"
   },
   "outputs": [],
   "source": [
    "class OptimizedVAE(nn.Module):\n",
    "    \"\"\"Optimized Variational Autoencoder for materials discovery with improved convergence.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int = 6, latent_dim: int = 5):\n",
    "        super(OptimizedVAE, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder - increased capacity for better convergence\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(32, latent_dim)\n",
    "        self.fc_var = nn.Linear(32, latent_dim)\n",
    "\n",
    "        # Decoder - symmetric to encoder, no sigmoid for unbounded features\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_dim)\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        log_var = self.fc_var(h)\n",
    "        return mu, log_var\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        reconstructed = self.decode(z)\n",
    "        return reconstructed, mu, log_var\n",
    "\n",
    "print(\"VAE class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-vae"
   },
   "outputs": [],
   "source": [
    "# Initialize and train the optimized VAE\n",
    "try:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    input_dim = features_scaled.shape[1]\n",
    "    model = OptimizedVAE(input_dim=input_dim, latent_dim=params['latent_dim']).to(device)\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    features_tensor = torch.FloatTensor(features_scaled)\n",
    "    dataset = torch.utils.data.TensorDataset(features_tensor, features_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # Training setup with improved hyperparameters\n",
    "    initial_lr = 0.005  # Higher initial learning rate for faster convergence\n",
    "    optimizer = optim.Adam(model.parameters(), lr=initial_lr)\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.995)  # Gradual decay\n",
    "    epochs = params['epochs']\n",
    "\n",
    "    print(f\"Training optimized VAE for {epochs} epochs on {device}...\")\n",
    "    print(f\"Model capacity: {model.input_dim} -> 64 -> 32 -> {model.latent_dim} -> 32 -> 64 -> {model.input_dim}\")\n",
    "    print(\"This may take a minute or two...\")\n",
    "\n",
    "    model.train()\n",
    "    losses = []\n",
    "    reconstruction_losses = []\n",
    "    kl_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        epoch_recon_loss = 0\n",
    "        epoch_kl_loss = 0\n",
    "        \n",
    "        for batch_x, _ in dataloader:\n",
    "            batch_x = batch_x.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            reconstructed, mu, log_var = model(batch_x)\n",
    "\n",
    "            # Compute losses with better weighting\n",
    "            reconstruction_loss = nn.functional.mse_loss(reconstructed, batch_x, reduction='sum')\n",
    "            kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "            \n",
    "            # KL annealing for better convergence\n",
    "            kl_weight = min(1.0, epoch / 10.0)  # Gradually increase KL weight\n",
    "            loss = reconstruction_loss + kl_weight * kl_loss\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_recon_loss += reconstruction_loss.item()\n",
    "            epoch_kl_loss += kl_loss.item()\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        avg_recon_loss = epoch_recon_loss / len(dataloader)\n",
    "        avg_kl_loss = epoch_kl_loss / len(dataloader)\n",
    "        \n",
    "        losses.append(avg_loss)\n",
    "        reconstruction_losses.append(avg_recon_loss)\n",
    "        kl_losses.append(avg_kl_loss)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Total Loss: {avg_loss:.4f}, Recon: {avg_recon_loss:.4f}, KL: {avg_kl_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "    print(\"\\nOptimized VAE training completed!\")\n",
    "    print(f\"Final loss: {losses[-1]:.4f} (improvement: {losses[0] - losses[-1]:.4f})\")\n",
    "\n",
    "    # Plot detailed training metrics\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    axes[0].plot(losses, label='Total Loss')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Total Training Loss')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].legend()\n",
    "    \n",
    "    axes[1].plot(reconstruction_losses, label='Reconstruction Loss', color='orange')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].set_title('Reconstruction Loss')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].legend()\n",
    "    \n",
    "    axes[2].plot(kl_losses, label='KL Divergence', color='green')\n",
    "    axes[2].set_xlabel('Epoch')\n",
    "    axes[2].set_ylabel('Loss')\n",
    "    axes[2].set_title('KL Divergence Loss')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    axes[2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "except RuntimeError as e:\n",
    "    if 'CUDA' in str(e):\n",
    "        print(\"ERROR: CUDA/GPU error occurred!\")\n",
    "        print(\"Try switching to CPU or reducing model size.\")\n",
    "        print(f\"Details: {e}\")\n",
    "    else:\n",
    "        print(f\"ERROR during training: {e}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during VAE setup/training: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "generate-materials"
   },
   "source": [
    "## Step 4: Generating New Materials\n",
    "\n",
    "Now that we have a trained VAE, we can generate new materials by sampling from the latent space. This is like asking the model to \"imagine\" new alloys that follow the patterns it learned from existing materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate-new-materials"
   },
   "outputs": [],
   "source": [
    "# Generate new materials\n",
    "model.eval()\n",
    "num_samples = params['num_samples']  # Controlled by slider\n",
    "\n",
    "print(f\"Generating {num_samples} new material compositions...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Sample from latent space\n",
    "    z = torch.randn(num_samples, model.latent_dim).to(device)\n",
    "    generated_features = model.decode(z).cpu().numpy()\n",
    "\n",
    "    # Inverse transform to original scale\n",
    "    generated_features = scaler.inverse_transform(generated_features)\n",
    "\n",
    "# Create DataFrame with generated materials\n",
    "elements = ['Al', 'Ti', 'V', 'Cr', 'Mn', 'Fe', 'Co', 'Ni', 'Cu', 'Zn']\n",
    "new_materials = []\n",
    "\n",
    "for i, features in enumerate(generated_features):\n",
    "    elem1, elem2 = random.sample(elements, 2)\n",
    "    comp1 = max(0.1, min(0.9, features[0]))\n",
    "    comp2 = 1.0 - comp1\n",
    "    \n",
    "    material = {\n",
    "        'id': f'generated_{i+1}',\n",
    "        'element_1': elem1,\n",
    "        'element_2': elem2,\n",
    "        'composition_1': comp1,\n",
    "        'composition_2': comp2,\n",
    "        'formula': f'{elem1}{comp1:.3f}{elem2}{comp2:.3f}',\n",
    "        'melting_point': abs(features[2]),\n",
    "        'density': abs(features[3]),\n",
    "        'electronegativity': max(0, features[4]),\n",
    "        'atomic_radius': max(0, features[5]),\n",
    "        'is_generated': True\n",
    "    }\n",
    "    new_materials.append(material)\n",
    "\n",
    "generated_df = pd.DataFrame(new_materials)\n",
    "print(f\"Generated {len(generated_df)} new materials!\")\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\nExample generated materials:\")\n",
    "generated_df[['formula', 'melting_point', 'density']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model-validation"
   },
   "source": [
    "## Step 5: Model Validation and Generation Analysis\n",
    "\n",
    "Let's evaluate the quality of our trained VAE model and analyze the diversity of the generated materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "validate-model"
   },
   "outputs": [],
   "source": [
    "# Evaluate model performance and generation quality\n",
    "model.eval()\n",
    "\n",
    "# 1. Reconstruction quality on training data\n",
    "print(\"=== MODEL VALIDATION ===\")\n",
    "with torch.no_grad():\n",
    "    train_reconstructed, _, _ = model(features_tensor.to(device))\n",
    "    train_reconstruction_error = nn.functional.mse_loss(train_reconstructed, features_tensor.to(device))\n",
    "    print(f\"Training reconstruction MSE: {train_reconstruction_error.item():.6f}\")\n",
    "\n",
    "# 2. Generation diversity analysis\n",
    "print(\"\\n=== GENERATION DIVERSITY ANALYSIS ===\")\n",
    "\n",
    "# Generate larger sample for analysis\n",
    "large_sample_size = min(1000, len(binary_data))\n",
    "with torch.no_grad():\n",
    "    z_large = torch.randn(large_sample_size, model.latent_dim).to(device)\n",
    "    generated_large = model.decode(z_large).cpu().numpy()\n",
    "    generated_large = scaler.inverse_transform(generated_large)\n",
    "\n",
    "# Calculate statistics\n",
    "original_stats = binary_data[['melting_point', 'density', 'electronegativity', 'atomic_radius']].describe()\n",
    "generated_stats = pd.DataFrame(generated_large[:, 2:], \n",
    "                              columns=['melting_point', 'density', 'electronegativity', 'atomic_radius']).describe()\n",
    "\n",
    "print(\"Original Materials Statistics:\")\n",
    "print(original_stats.loc[['mean', 'std']].round(3))\n",
    "print(\"\\nGenerated Materials Statistics:\")\n",
    "print(generated_stats.loc[['mean', 'std']].round(3))\n",
    "\n",
    "# Coverage analysis - how well generated materials cover the original distribution\n",
    "\n",
    "properties = ['melting_point', 'density', 'electronegativity', 'atomic_radius']\n",
    "coverage_scores = {}\n",
    "\n",
    "for prop in properties:\n",
    "    original_vals = binary_data[prop].values\n",
    "    generated_vals = generated_large[:, feature_cols.index(prop)]\n",
    "    \n",
    "    # Kolmogorov-Smirnov test for distribution similarity\n",
    "    ks_stat, p_value = ks_2samp(original_vals, generated_vals)\n",
    "    coverage_scores[prop] = {'ks_stat': ks_stat, 'p_value': p_value}\n",
    "    \n",
    "print(\"\\nDistribution Similarity (KS Test):\")\n",
    "for prop, scores in coverage_scores.items():\n",
    "    print(f\"{prop}: KS-stat={scores['ks_stat']:.3f}, p-value={scores['p_value']:.3f}\")\n",
    "\n",
    "# Novelty check - how many generated materials are outside training range\n",
    "novelty_count = 0\n",
    "for i, gen_features in enumerate(generated_large):\n",
    "    is_novel = False\n",
    "    for j, prop in enumerate(properties):\n",
    "        prop_idx = feature_cols.index(prop)\n",
    "        gen_val = gen_features[prop_idx]\n",
    "        orig_min, orig_max = binary_data[prop].min(), binary_data[prop].max()\n",
    "        if gen_val < orig_min * 0.9 or gen_val > orig_max * 1.1:  # 10% margin\n",
    "            is_novel = True\n",
    "    if is_novel:\n",
    "        novelty_count += 1\n",
    "\n",
    "print(f\"\\nNovelty Analysis: {novelty_count}/{large_sample_size} ({novelty_count/large_sample_size*100:.1f}%) generated materials extend beyond training range\")\n",
    "\n",
    "# Latent space analysis\n",
    "print(\"\\n=== LATENT SPACE ANALYSIS ===\")\n",
    "with torch.no_grad():\n",
    "    _, mu, log_var = model(features_tensor.to(device))\n",
    "    mu_np = mu.cpu().numpy()\n",
    "    log_var_np = log_var.cpu().numpy()\n",
    "\n",
    "print(f\"Latent space dimension: {model.latent_dim}\")\n",
    "print(f\"Latent means - Mean: {mu_np.mean():.3f}, Std: {mu_np.std():.3f}\")\n",
    "print(f\"Latent variances - Mean: {log_var_np.mean():.3f}, Std: {log_var_np.std():.3f}\")\n",
    "\n",
    "# Plot latent space if 2D\n",
    "if model.latent_dim == 2:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(mu_np[:, 0], mu_np[:, 1], alpha=0.6, c='blue', s=30)\n",
    "    plt.xlabel('Latent Dimension 1')\n",
    "    plt.ylabel('Latent Dimension 2')\n",
    "    plt.title('Training Data in Latent Space')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nModel validation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "materials-clustering"
   },
   "source": [
    "## Step 6: Materials Clustering\n",
    "\n",
    "Let's analyze the original materials by grouping them into clusters. This helps us understand natural groupings in the materials space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cluster-analysis"
   },
   "outputs": [],
   "source": [
    "# Perform clustering analysis\n",
    "pca = PCA(n_components=2)\n",
    "features_pca = pca.fit_transform(features_scaled)\n",
    "\n",
    "# Find optimal number of clusters\n",
    "silhouette_scores = []\n",
    "for n_clusters in range(2, 8):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(features_pca)\n",
    "    silhouette_avg = silhouette_score(features_pca, cluster_labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "optimal_clusters = silhouette_scores.index(max(silhouette_scores)) + 2\n",
    "print(f\"Optimal number of clusters: {optimal_clusters}\")\n",
    "\n",
    "# Perform clustering\n",
    "kmeans = KMeans(n_clusters=optimal_clusters, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(features_pca)\n",
    "\n",
    "# Add cluster info to data\n",
    "clustered_data = binary_data.copy()\n",
    "clustered_data['cluster'] = cluster_labels\n",
    "clustered_data['pca_1'] = features_pca[:, 0]\n",
    "clustered_data['pca_2'] = features_pca[:, 1]\n",
    "\n",
    "print(f\"Materials grouped into {optimal_clusters} clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualizations"
   },
   "source": [
    "## Step 7: Visualizations and Analysis\n",
    "\n",
    "Let's create visualizations to compare original and generated materials, and explore the clustering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-visualizations"
   },
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Materials Discovery ML Workshop Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Melting Point vs Density\n",
    "ax = axes[0, 0]\n",
    "ax.scatter(binary_data['density'], binary_data['melting_point'], \n",
    "           alpha=0.6, c='blue', label='Original Materials', s=30)\n",
    "ax.scatter(generated_df['density'], generated_df['melting_point'], \n",
    "           alpha=0.8, c='red', marker='x', label='Generated Materials', s=50)\n",
    "ax.set_xlabel('Density (g/cm\u00c2\u00b3)')\n",
    "ax.set_ylabel('Melting Point (K)')\n",
    "ax.set_title('Material Properties: Original vs Generated')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Composition Space\n",
    "ax = axes[0, 1]\n",
    "ax.scatter(binary_data['composition_1'], binary_data['composition_2'], \n",
    "           alpha=0.6, c='blue', label='Original', s=30)\n",
    "ax.scatter(generated_df['composition_1'], generated_df['composition_2'], \n",
    "           alpha=0.8, c='red', marker='x', label='Generated', s=50)\n",
    "ax.set_xlabel('Element 1 Composition')\n",
    "ax.set_ylabel('Element 2 Composition')\n",
    "ax.set_title('Composition Space')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Cluster Analysis\n",
    "ax = axes[0, 2]\n",
    "scatter = ax.scatter(clustered_data['pca_1'], clustered_data['pca_2'], \n",
    "                     c=clustered_data['cluster'], cmap='viridis', alpha=0.7, s=40)\n",
    "ax.set_xlabel('PCA Component 1')\n",
    "ax.set_ylabel('PCA Component 2')\n",
    "ax.set_title('Material Clusters (PCA)')\n",
    "plt.colorbar(scatter, ax=ax, label='Cluster')\n",
    "\n",
    "# 4. Property Distributions - Melting Point\n",
    "ax = axes[1, 0]\n",
    "ax.hist(binary_data['melting_point'], bins=20, alpha=0.7, color='blue', \n",
    "        label='Original', density=True)\n",
    "ax.hist(generated_df['melting_point'], bins=20, alpha=0.7, color='red', \n",
    "        label='Generated', density=True)\n",
    "ax.set_xlabel('Melting Point (K)')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Melting Point Distribution')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Property Distributions - Density\n",
    "ax = axes[1, 1]\n",
    "ax.hist(binary_data['density'], bins=20, alpha=0.7, color='blue', \n",
    "        label='Original', density=True)\n",
    "ax.hist(generated_df['density'], bins=20, alpha=0.7, color='red', \n",
    "        label='Generated', density=True)\n",
    "ax.set_xlabel('Density (g/cm\u00c2\u00b3)')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Density Distribution')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Generated Materials Table\n",
    "ax = axes[1, 2]\n",
    "ax.axis('off')\n",
    "table_data = generated_df.head(8)[['formula', 'melting_point', 'density']].copy()\n",
    "table_data['melting_point'] = table_data['melting_point'].round(1)\n",
    "table_data['density'] = table_data['density'].round(3)\n",
    "table = ax.table(cellText=table_data.values, colLabels=table_data.columns, \n",
    "                 cellLoc='center', loc='center', bbox=[0, 0, 1, 1])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(9)\n",
    "table.scale(1, 1.5)\n",
    "ax.set_title('Sample Generated Materials', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('materials_discovery_workshop.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualization saved as 'materials_discovery_workshop.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "export-results"
   },
   "source": [
    "## Step 8: Export Results\n",
    "\n",
    "Let's save our generated materials for further analysis or fabrication testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save-results"
   },
   "outputs": [],
   "source": [
    "# Save generated materials\n",
    "generated_df.to_csv('generated_materials_workshop.csv', index=False)\n",
    "print(\"Generated materials saved to 'generated_materials_workshop.csv'\")\n",
    "\n",
    "# Workshop summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MATERIALS DISCOVERY WORKSHOP SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Original materials analyzed: {len(binary_data)}\")\n",
    "print(f\"New materials generated: {len(generated_df)}\")\n",
    "print(f\"Material clusters identified: {optimal_clusters}\")\n",
    "print()\n",
    "print(\"Key Takeaways:\")\n",
    "print(\"- ML can learn complex patterns in materials data\")\n",
    "print(\"- VAE models can generate novel material compositions\")\n",
    "print(\"- Clustering reveals natural groupings in materials space\")\n",
    "print(\"- This approach can accelerate materials R&D workflows\")\n",
    "print()\n",
    "print(\"Next Steps:\")\n",
    "print(\"- Validate generated materials experimentally\")\n",
    "print(\"- Extend to ternary and higher-order alloys\")\n",
    "print(\"- Incorporate additional material properties\")\n",
    "print(\"- Use reinforcement learning for property optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "advanced-validation"
   },
   "source": [
    "## Step 9: Advanced Validation Techniques\n",
    "\n",
    "For production-ready material discovery, we implement comprehensive validation techniques including cross-validation stability testing, baseline model comparisons, robustness testing, and domain-specific validation using materials science principles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "comprehensive-validation"
   },
   "outputs": [],
   "source": [
    "# Advanced validation techniques for material discovery\n",
    "\n",
    "print(\"=== ADVANCED VALIDATION TECHNIQUES ===\")\n",
    "\n",
    "# 1. Cross-validation stability test\n",
    "print(\"\\n1. CROSS-VALIDATION STABILITY TEST\")\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import ks_2samp\n",
    "import numpy as np\n",
    "\n",
    "def cross_validate_vae(features_scaled, latent_dim=5, epochs=20, folds=3):\n",
    "    \"\"\"Test VAE stability across different data splits\"\"\"\n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "    cv_losses = []\n",
    "    cv_generation_diversity = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(features_scaled)):\n",
    "        print(f\"  Training fold {fold+1}/{folds}...\")\n",
    "        \n",
    "        # Split data\n",
    "        train_data = features_scaled[train_idx]\n",
    "        val_data = features_scaled[val_idx]\n",
    "        \n",
    "        # Quick training on this fold\n",
    "        model_fold = OptimizedVAE(input_dim=features_scaled.shape[1], latent_dim=latent_dim).to(device)\n",
    "        optimizer = optim.Adam(model_fold.parameters(), lr=0.005)\n",
    "        \n",
    "        train_tensor = torch.FloatTensor(train_data)\n",
    "        val_tensor = torch.FloatTensor(val_data)\n",
    "        \n",
    "        model_fold.train()\n",
    "        for epoch in range(epochs):\n",
    "            reconstructed, mu, log_var = model_fold(train_tensor.to(device))\n",
    "            reconstruction_loss = nn.functional.mse_loss(reconstructed, train_tensor.to(device), reduction='sum')\n",
    "            kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "            kl_weight = min(1.0, epoch / 10.0)\n",
    "            loss = reconstruction_loss + kl_weight * kl_loss\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Evaluate reconstruction on validation set\n",
    "        model_fold.eval()\n",
    "        with torch.no_grad():\n",
    "            val_reconstructed, _, _ = model_fold(val_tensor.to(device))\n",
    "            val_loss = nn.functional.mse_loss(val_reconstructed, val_tensor.to(device))\n",
    "            cv_losses.append(val_loss.item())\n",
    "            \n",
    "            # Test generation diversity\n",
    "            z_test = torch.randn(100, latent_dim).to(device)\n",
    "            generated_test = model_fold.decode(z_test).cpu().numpy()\n",
    "            generated_test = scaler.inverse_transform(generated_test)\n",
    "            cv_generation_diversity.append(np.std(generated_test, axis=0).mean())\n",
    "    \n",
    "    print(f\"Cross-validation reconstruction MSE: {np.mean(cv_losses):.4f} \u00c2\u00b1 {np.std(cv_losses):.4f}\")\n",
    "    print(f\"Generation diversity stability: {np.mean(cv_generation_diversity):.4f} \u00c2\u00b1 {np.std(cv_generation_diversity):.4f}\")\n",
    "    \n",
    "    return cv_losses, cv_generation_diversity\n",
    "\n",
    "# Run cross-validation\n",
    "cv_losses, cv_diversity = cross_validate_vae(features_scaled, epochs=10, folds=3)\n",
    "\n",
    "# 2. Baseline model comparison\n",
    "print(\"\\n2. BASELINE MODEL COMPARISON\")\n",
    "\n",
    "def comprehensive_baseline_comparison(features_scaled, binary_data):\n",
    "    \"\"\"Compare VAE against multiple baseline approaches\"\"\"\n",
    "    \n",
    "    # PCA baseline\n",
    "    pca = PCA(n_components=5)\n",
    "    pca_reconstructed = pca.inverse_transform(pca.fit_transform(features_scaled))\n",
    "    pca_mse = np.mean((features_scaled - pca_reconstructed) ** 2)\n",
    "    pca_explained_var = pca.explained_variance_ratio_.sum()\n",
    "    \n",
    "    # Random Forest baseline\n",
    "    rf_predictions = []\n",
    "    for i, col in enumerate(['melting_point', 'density', 'electronegativity', 'atomic_radius']):\n",
    "        feature_idx = feature_cols.index(col)\n",
    "        X = np.delete(features_scaled, feature_idx, axis=1)\n",
    "        y = features_scaled[:, feature_idx]\n",
    "        \n",
    "        rf = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "        rf.fit(X, y)\n",
    "        pred = rf.predict(X)\n",
    "        rf_predictions.append(pred)\n",
    "    \n",
    "    rf_reconstructed = features_scaled.copy()\n",
    "    for i in range(4):\n",
    "        feature_idx = feature_cols.index(['melting_point', 'density', 'electronegativity', 'atomic_radius'][i])\n",
    "        rf_reconstructed[:, feature_idx] = rf_predictions[i]\n",
    "    \n",
    "    rf_mse = np.mean((features_scaled - rf_reconstructed) ** 2)\n",
    "    \n",
    "    # VAE reconstruction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        vae_reconstructed, _, _ = model(features_tensor.to(device))\n",
    "        vae_reconstructed = vae_reconstructed.cpu().numpy()\n",
    "        vae_mse = np.mean((features_scaled - vae_reconstructed) ** 2)\n",
    "    \n",
    "    print(f\"PCA Reconstruction MSE: {pca_mse:.4f} (explained variance: {pca_explained_var:.3f})\")\n",
    "    print(f\"Random Forest Reconstruction MSE: {rf_mse:.4f}\")\n",
    "    print(f\"VAE Reconstruction MSE: {vae_mse:.4f}\")\n",
    "    print(f\"VAE improvement over PCA: {(pca_mse - vae_mse) / pca_mse * 100:.1f}%\")\n",
    "    print(f\"VAE improvement over RF: {(rf_mse - vae_mse) / rf_mse * 100:.1f}%\")\n",
    "    \n",
    "    # Generation capability comparison\n",
    "    print(\"\\nGeneration Capability Analysis:\")\n",
    "    \n",
    "    # PCA can only interpolate, not extrapolate\n",
    "    pca_min, pca_max = np.min(pca_reconstructed, axis=0), np.max(pca_reconstructed, axis=0)\n",
    "    original_min, original_max = np.min(features_scaled, axis=0), np.max(features_scaled, axis=0)\n",
    "    pca_coverage = np.mean((pca_max - pca_min) / (original_max - original_min))\n",
    "    \n",
    "    # VAE generation range\n",
    "    large_sample_size = 1000\n",
    "    with torch.no_grad():\n",
    "        z_large = torch.randn(large_sample_size, model.latent_dim).to(device)\n",
    "        vae_generated = model.decode(z_large).cpu().numpy()\n",
    "    \n",
    "    vae_min, vae_max = np.min(vae_generated, axis=0), np.max(vae_generated, axis=0)\n",
    "    vae_coverage = np.mean((vae_max - vae_min) / (original_max - original_min))\n",
    "    \n",
    "    print(f\"PCA generation coverage (relative to training range): {pca_coverage:.3f}\")\n",
    "    print(f\"VAE generation coverage (relative to training range): {vae_coverage:.3f}\")\n",
    "    print(f\"VAE expands generation space by {vae_coverage/pca_coverage:.1f}x compared to PCA\")\n",
    "    \n",
    "    return {\n",
    "        'pca_mse': pca_mse, 'rf_mse': rf_mse, 'vae_mse': vae_mse,\n",
    "        'pca_coverage': pca_coverage, 'vae_coverage': vae_coverage\n",
    "    }\n",
    "\n",
    "baseline_results = comprehensive_baseline_comparison(features_scaled, binary_data)\n",
    "\n",
    "print(\"\\nAdvanced validation completed successfully!\")\n",
    "print(\"\u00f0\u0178\u017d\u2030 The enhanced workshop with comprehensive validation is ready for Google Colab!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}